{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f494c3b0-8c93-4674-a4a8-f4e85d1d062b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'sd-scripts' already exists and is not an empty directory.\n",
      "/home/ec2-user/SageMaker/sd-scripts\n",
      "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
      "Collecting torch==2.0.1+cu118\n",
      "  Downloading https://download.pytorch.org/whl/cu118/torch-2.0.1%2Bcu118-cp310-cp310-linux_x86_64.whl (2267.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 GB\u001b[0m \u001b[31m458.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting torchvision==0.15.2+cu118\n",
      "  Downloading https://download.pytorch.org/whl/cu118/torchvision-0.15.2%2Bcu118-cp310-cp310-linux_x86_64.whl (6.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.1/6.1 MB\u001b[0m \u001b[31m55.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch==2.0.1+cu118) (3.12.2)\n",
      "Requirement already satisfied: typing-extensions in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch==2.0.1+cu118) (4.7.1)\n",
      "Requirement already satisfied: sympy in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch==2.0.1+cu118) (1.12)\n",
      "Requirement already satisfied: networkx in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch==2.0.1+cu118) (3.1)\n",
      "Requirement already satisfied: jinja2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch==2.0.1+cu118) (3.1.2)\n",
      "Requirement already satisfied: triton==2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch==2.0.1+cu118) (2.0.0)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torchvision==0.15.2+cu118) (1.24.4)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torchvision==0.15.2+cu118) (2.31.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torchvision==0.15.2+cu118) (10.0.0)\n",
      "Collecting cmake (from triton==2.0.0->torch==2.0.1+cu118)\n",
      "  Downloading https://download.pytorch.org/whl/cmake-3.25.0-py2.py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting lit (from triton==2.0.0->torch==2.0.1+cu118)\n",
      "  Downloading https://download.pytorch.org/whl/lit-15.0.7.tar.gz (132 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.3/132.3 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jinja2->torch==2.0.1+cu118) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->torchvision==0.15.2+cu118) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->torchvision==0.15.2+cu118) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->torchvision==0.15.2+cu118) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->torchvision==0.15.2+cu118) (2023.5.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sympy->torch==2.0.1+cu118) (1.3.0)\n",
      "Building wheels for collected packages: lit\n",
      "  Building wheel for lit (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for lit: filename=lit-15.0.7-py3-none-any.whl size=89988 sha256=a7804cca7f925ccba7374ac81c9f5deaa0590873152436e4ec7166853e247e4e\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/bf/62/0d/01218f13f6a8051e982a7ce31d12b7bfd725dc69bd227ae104\n",
      "Successfully built lit\n",
      "Installing collected packages: lit, cmake, torch, torchvision\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.0.1\n",
      "    Uninstalling torch-2.0.1:\n",
      "      Successfully uninstalled torch-2.0.1\n",
      "  Attempting uninstall: torchvision\n",
      "    Found existing installation: torchvision 0.15.2\n",
      "    Uninstalling torchvision-0.15.2:\n",
      "      Successfully uninstalled torchvision-0.15.2\n",
      "Successfully installed cmake-3.25.0 lit-15.0.7 torch-2.0.1+cu118 torchvision-0.15.2+cu118\n",
      "Obtaining file:///home/ec2-user/SageMaker/sd-scripts (from -r requirements.txt (line 29))\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting accelerate==0.19.0 (from -r requirements.txt (line 1))\n",
      "  Downloading accelerate-0.19.0-py3-none-any.whl (219 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m219.1/219.1 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting transformers==4.30.2 (from -r requirements.txt (line 2))\n",
      "  Obtaining dependency information for transformers==4.30.2 from https://files.pythonhosted.org/packages/5b/0b/e45d26ccd28568013523e04f325432ea88a442b4e3020b757cf4361f0120/transformers-4.30.2-py3-none-any.whl.metadata\n",
      "  Downloading transformers-4.30.2-py3-none-any.whl.metadata (113 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m113.6/113.6 kB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting diffusers[torch]==0.19.3 (from -r requirements.txt (line 3))\n",
      "  Obtaining dependency information for diffusers[torch]==0.19.3 from https://files.pythonhosted.org/packages/f6/a8/20b5fddffa832386c51e5a536a08e0a4024ed5c14c9e0758d422cb562895/diffusers-0.19.3-py3-none-any.whl.metadata\n",
      "  Downloading diffusers-0.19.3-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting ftfy==6.1.1 (from -r requirements.txt (line 4))\n",
      "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting opencv-python==4.7.0.68 (from -r requirements.txt (line 6))\n",
      "  Downloading opencv_python-4.7.0.68-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (61.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.8/61.8 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting einops==0.6.0 (from -r requirements.txt (line 7))\n",
      "  Downloading einops-0.6.0-py3-none-any.whl (41 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.6/41.6 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pytorch-lightning==1.9.0 (from -r requirements.txt (line 8))\n",
      "  Downloading pytorch_lightning-1.9.0-py3-none-any.whl (825 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m825.8/825.8 kB\u001b[0m \u001b[31m91.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tensorboard==2.11.0 (from -r requirements.txt (line 10))\n",
      "  Downloading tensorboard-2.11.0-py3-none-any.whl (6.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m135.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting safetensors==0.3.1 (from -r requirements.txt (line 11))\n",
      "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m119.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting altair==4.2.2 (from -r requirements.txt (line 13))\n",
      "  Downloading altair-4.2.2-py3-none-any.whl (813 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m813.6/813.6 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting easygui==0.98.3 (from -r requirements.txt (line 14))\n",
      "  Downloading easygui-0.98.3-py2.py3-none-any.whl (92 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.7/92.7 kB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting toml==0.10.2 (from -r requirements.txt (line 15))\n",
      "  Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Collecting voluptuous==0.13.1 (from -r requirements.txt (line 16))\n",
      "  Downloading voluptuous-0.13.1-py3-none-any.whl (29 kB)\n",
      "Collecting huggingface-hub==0.15.1 (from -r requirements.txt (line 17))\n",
      "  Obtaining dependency information for huggingface-hub==0.15.1 from https://files.pythonhosted.org/packages/62/a2/8a416d167216403ceeef3aaf8c22b0c61b1ae571644473d67eb7fecbb69e/huggingface_hub-0.15.1-py3-none-any.whl.metadata\n",
      "  Downloading huggingface_hub-0.15.1-py3-none-any.whl.metadata (8.0 kB)\n",
      "Collecting invisible-watermark==0.2.0 (from -r requirements.txt (line 19))\n",
      "  Obtaining dependency information for invisible-watermark==0.2.0 from https://files.pythonhosted.org/packages/2b/57/18b5a914f6d7994dd349252873169e946dc824328e9a37fd15ed836deedc/invisible_watermark-0.2.0-py3-none-any.whl.metadata\n",
      "  Downloading invisible_watermark-0.2.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting open-clip-torch==2.20.0 (from -r requirements.txt (line 27))\n",
      "  Obtaining dependency information for open-clip-torch==2.20.0 from https://files.pythonhosted.org/packages/70/b9/a9f2c37f998c20be57fed0128934f4a311c6596c1f6b9c2fe358b26125bc/open_clip_torch-2.20.0-py3-none-any.whl.metadata\n",
      "  Downloading open_clip_torch-2.20.0-py3-none-any.whl.metadata (46 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.8/46.8 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from accelerate==0.19.0->-r requirements.txt (line 1)) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from accelerate==0.19.0->-r requirements.txt (line 1)) (21.3)\n",
      "Requirement already satisfied: psutil in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from accelerate==0.19.0->-r requirements.txt (line 1)) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from accelerate==0.19.0->-r requirements.txt (line 1)) (6.0)\n",
      "Requirement already satisfied: torch>=1.6.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from accelerate==0.19.0->-r requirements.txt (line 1)) (2.0.1+cu118)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers==4.30.2->-r requirements.txt (line 2)) (3.12.2)\n",
      "Collecting regex!=2019.12.17 (from transformers==4.30.2->-r requirements.txt (line 2))\n",
      "  Obtaining dependency information for regex!=2019.12.17 from https://files.pythonhosted.org/packages/d1/df/460ca6171a8494fcf37af43f52f6fac23e38784bb4a26563f6fa01ef6faf/regex-2023.8.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading regex-2023.8.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers==4.30.2->-r requirements.txt (line 2)) (2.31.0)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.30.2->-r requirements.txt (line 2))\n",
      "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m51.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers==4.30.2->-r requirements.txt (line 2)) (4.65.0)\n",
      "Requirement already satisfied: importlib-metadata in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from diffusers[torch]==0.19.3->-r requirements.txt (line 3)) (6.8.0)\n",
      "Requirement already satisfied: Pillow in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from diffusers[torch]==0.19.3->-r requirements.txt (line 3)) (10.0.0)\n",
      "Requirement already satisfied: wcwidth>=0.2.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from ftfy==6.1.1->-r requirements.txt (line 4)) (0.2.6)\n",
      "Requirement already satisfied: fsspec[http]>2021.06.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pytorch-lightning==1.9.0->-r requirements.txt (line 8)) (2023.6.0)\n",
      "Collecting torchmetrics>=0.7.0 (from pytorch-lightning==1.9.0->-r requirements.txt (line 8))\n",
      "  Obtaining dependency information for torchmetrics>=0.7.0 from https://files.pythonhosted.org/packages/e3/86/47091c33ecf05f8826d134fd518485d4c68ca524c053b2fdd4e041c20547/torchmetrics-1.1.1-py3-none-any.whl.metadata\n",
      "  Downloading torchmetrics-1.1.1-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pytorch-lightning==1.9.0->-r requirements.txt (line 8)) (4.7.1)\n",
      "Collecting lightning-utilities>=0.4.2 (from pytorch-lightning==1.9.0->-r requirements.txt (line 8))\n",
      "  Obtaining dependency information for lightning-utilities>=0.4.2 from https://files.pythonhosted.org/packages/46/ee/8641eeb6a062f383b7d6875604e1f3f83bd2c93a0b4dbcabd3150b32de6e/lightning_utilities-0.9.0-py3-none-any.whl.metadata\n",
      "  Downloading lightning_utilities-0.9.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting absl-py>=0.4 (from tensorboard==2.11.0->-r requirements.txt (line 10))\n",
      "  Downloading absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.5/126.5 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting grpcio>=1.24.3 (from tensorboard==2.11.0->-r requirements.txt (line 10))\n",
      "  Obtaining dependency information for grpcio>=1.24.3 from https://files.pythonhosted.org/packages/28/fa/c38a010d3fffcac07ef121abb34eb2c3db0876df74267ce5bde13c3a6ed7/grpcio-1.57.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading grpcio-1.57.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
      "Collecting google-auth<3,>=1.6.3 (from tensorboard==2.11.0->-r requirements.txt (line 10))\n",
      "  Obtaining dependency information for google-auth<3,>=1.6.3 from https://files.pythonhosted.org/packages/9c/8d/bff87fc722553a5691d8514da5523c23547f3894189ba03b57592e37bdc2/google_auth-2.22.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading google_auth-2.22.0-py2.py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard==2.11.0->-r requirements.txt (line 10))\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Collecting markdown>=2.6.8 (from tensorboard==2.11.0->-r requirements.txt (line 10))\n",
      "  Obtaining dependency information for markdown>=2.6.8 from https://files.pythonhosted.org/packages/1a/b5/228c1cdcfe138f1a8e01ab1b54284c8b83735476cb22b6ba251656ed13ad/Markdown-3.4.4-py3-none-any.whl.metadata\n",
      "  Downloading Markdown-3.4.4-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting protobuf<4,>=3.9.2 (from tensorboard==2.11.0->-r requirements.txt (line 10))\n",
      "  Downloading protobuf-3.20.3-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: setuptools>=41.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from tensorboard==2.11.0->-r requirements.txt (line 10)) (68.0.0)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0 (from tensorboard==2.11.0->-r requirements.txt (line 10))\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hCollecting tensorboard-plugin-wit>=1.6.0 (from tensorboard==2.11.0->-r requirements.txt (line 10))\n",
      "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.3/781.3 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: werkzeug>=1.0.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from tensorboard==2.11.0->-r requirements.txt (line 10)) (2.3.6)\n",
      "Requirement already satisfied: wheel>=0.26 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from tensorboard==2.11.0->-r requirements.txt (line 10)) (0.40.0)\n",
      "Requirement already satisfied: entrypoints in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from altair==4.2.2->-r requirements.txt (line 13)) (0.4)\n",
      "Requirement already satisfied: jinja2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from altair==4.2.2->-r requirements.txt (line 13)) (3.1.2)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from altair==4.2.2->-r requirements.txt (line 13)) (4.18.4)\n",
      "Requirement already satisfied: pandas>=0.18 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from altair==4.2.2->-r requirements.txt (line 13)) (1.5.3)\n",
      "Collecting toolz (from altair==4.2.2->-r requirements.txt (line 13))\n",
      "  Downloading toolz-0.12.0-py3-none-any.whl (55 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.8/55.8 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting PyWavelets>=1.1.1 (from invisible-watermark==0.2.0->-r requirements.txt (line 19))\n",
      "  Downloading PyWavelets-1.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torchvision in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from open-clip-torch==2.20.0->-r requirements.txt (line 27)) (0.15.2+cu118)\n",
      "Collecting sentencepiece (from open-clip-torch==2.20.0->-r requirements.txt (line 27))\n",
      "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hCollecting timm (from open-clip-torch==2.20.0->-r requirements.txt (line 27))\n",
      "  Obtaining dependency information for timm from https://files.pythonhosted.org/packages/35/57/84a71b5b540152fb5f5ec7a34f4fcd7fafa4e21053098146c831874e29e5/timm-0.9.6-py3-none-any.whl.metadata\n",
      "  Downloading timm-0.9.6-py3-none-any.whl.metadata (58 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.6/58.6 kB\u001b[0m \u001b[31m999.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]>2021.06.0->pytorch-lightning==1.9.0->-r requirements.txt (line 8))\n",
      "  Obtaining dependency information for aiohttp!=4.0.0a0,!=4.0.0a1 from https://files.pythonhosted.org/packages/3e/f6/fcda07dd1e72260989f0b22dde999ecfe80daa744f23ca167083683399bc/aiohttp-3.8.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading aiohttp-3.8.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth<3,>=1.6.3->tensorboard==2.11.0->-r requirements.txt (line 10))\n",
      "  Obtaining dependency information for cachetools<6.0,>=2.0.0 from https://files.pythonhosted.org/packages/a9/c9/c8a7710f2cedcb1db9224fdd4d8307c9e48cbddc46c18b515fefc0f1abbe/cachetools-5.3.1-py3-none-any.whl.metadata\n",
      "  Downloading cachetools-5.3.1-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth<3,>=1.6.3->tensorboard==2.11.0->-r requirements.txt (line 10))\n",
      "  Downloading pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.3/181.3 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: rsa<5,>=3.1.4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard==2.11.0->-r requirements.txt (line 10)) (4.7.2)\n",
      "Requirement already satisfied: six>=1.9.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard==2.11.0->-r requirements.txt (line 10)) (1.16.0)\n",
      "Requirement already satisfied: urllib3<2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard==2.11.0->-r requirements.txt (line 10)) (1.26.14)\n",
      "Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard==2.11.0->-r requirements.txt (line 10))\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jsonschema>=3.0->altair==4.2.2->-r requirements.txt (line 13)) (23.1.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jsonschema>=3.0->altair==4.2.2->-r requirements.txt (line 13)) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jsonschema>=3.0->altair==4.2.2->-r requirements.txt (line 13)) (0.30.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jsonschema>=3.0->altair==4.2.2->-r requirements.txt (line 13)) (0.9.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from packaging>=20.0->accelerate==0.19.0->-r requirements.txt (line 1)) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pandas>=0.18->altair==4.2.2->-r requirements.txt (line 13)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pandas>=0.18->altair==4.2.2->-r requirements.txt (line 13)) (2023.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers==4.30.2->-r requirements.txt (line 2)) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers==4.30.2->-r requirements.txt (line 2)) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers==4.30.2->-r requirements.txt (line 2)) (2023.5.7)\n",
      "Requirement already satisfied: sympy in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=1.6.0->accelerate==0.19.0->-r requirements.txt (line 1)) (1.12)\n",
      "Requirement already satisfied: networkx in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=1.6.0->accelerate==0.19.0->-r requirements.txt (line 1)) (3.1)\n",
      "Requirement already satisfied: triton==2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=1.6.0->accelerate==0.19.0->-r requirements.txt (line 1)) (2.0.0)\n",
      "Requirement already satisfied: cmake in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.6.0->accelerate==0.19.0->-r requirements.txt (line 1)) (3.25.0)\n",
      "Requirement already satisfied: lit in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.6.0->accelerate==0.19.0->-r requirements.txt (line 1)) (15.0.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard==2.11.0->-r requirements.txt (line 10)) (2.1.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from importlib-metadata->diffusers[torch]==0.19.3->-r requirements.txt (line 3)) (3.16.2)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning==1.9.0->-r requirements.txt (line 8))\n",
      "  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning==1.9.0->-r requirements.txt (line 8))\n",
      "  Obtaining dependency information for async-timeout<5.0,>=4.0.0a3 from https://files.pythonhosted.org/packages/a7/fa/e01228c2938de91d47b307831c62ab9e4001e747789d0b05baf779a6488c/async_timeout-4.0.3-py3-none-any.whl.metadata\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning==1.9.0->-r requirements.txt (line 8))\n",
      "  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning==1.9.0->-r requirements.txt (line 8))\n",
      "  Obtaining dependency information for frozenlist>=1.1.1 from https://files.pythonhosted.org/packages/1e/28/74b8b6451c89c070d34e753d8b65a1e4ce508a6808b18529f36e8c0e2184/frozenlist-1.4.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading frozenlist-1.4.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning==1.9.0->-r requirements.txt (line 8))\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard==2.11.0->-r requirements.txt (line 10)) (0.4.8)\n",
      "Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard==2.11.0->-r requirements.txt (line 10))\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m151.7/151.7 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sympy->torch>=1.6.0->accelerate==0.19.0->-r requirements.txt (line 1)) (1.3.0)\n",
      "Downloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading invisible_watermark-0.2.0-py3-none-any.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading open_clip_torch-2.20.0-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading google_auth-2.22.0-py2.py3-none-any.whl (181 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.8/181.8 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading grpcio-1.57.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m47.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading lightning_utilities-0.9.0-py3-none-any.whl (23 kB)\n",
      "Downloading Markdown-3.4.4-py3-none-any.whl (94 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.2/94.2 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2023.8.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (771 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m771.9/771.9 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading torchmetrics-1.1.1-py3-none-any.whl (763 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m763.4/763.4 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading diffusers-0.19.3-py3-none-any.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading timm-0.9.6-py3-none-any.whl (2.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading aiohttp-3.8.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading cachetools-5.3.1-py3-none-any.whl (9.3 kB)\n",
      "Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Downloading frozenlist-1.4.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (225 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m225.7/225.7 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: voluptuous, tokenizers, tensorboard-plugin-wit, sentencepiece, safetensors, library, easygui, toolz, toml, tensorboard-data-server, regex, PyWavelets, pyasn1-modules, protobuf, opencv-python, oauthlib, multidict, markdown, grpcio, ftfy, frozenlist, einops, cachetools, async-timeout, absl-py, yarl, requests-oauthlib, lightning-utilities, huggingface-hub, google-auth, aiosignal, transformers, google-auth-oauthlib, diffusers, aiohttp, tensorboard, altair, torchmetrics, timm, accelerate, pytorch-lightning, open-clip-torch, invisible-watermark\n",
      "  Running setup.py develop for library\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 4.23.4\n",
      "    Uninstalling protobuf-4.23.4:\n",
      "      Successfully uninstalled protobuf-4.23.4\n",
      "  Attempting uninstall: opencv-python\n",
      "    Found existing installation: opencv-python 4.6.0.66\n",
      "    Uninstalling opencv-python-4.6.0.66:\n",
      "      Successfully uninstalled opencv-python-4.6.0.66\n",
      "Successfully installed PyWavelets-1.4.1 absl-py-1.4.0 accelerate-0.19.0 aiohttp-3.8.5 aiosignal-1.3.1 altair-4.2.2 async-timeout-4.0.3 cachetools-5.3.1 diffusers-0.19.3 easygui-0.98.3 einops-0.6.0 frozenlist-1.4.0 ftfy-6.1.1 google-auth-2.22.0 google-auth-oauthlib-0.4.6 grpcio-1.57.0 huggingface-hub-0.15.1 invisible-watermark-0.2.0 library-0.0.0 lightning-utilities-0.9.0 markdown-3.4.4 multidict-6.0.4 oauthlib-3.2.2 open-clip-torch-2.20.0 opencv-python-4.7.0.68 protobuf-3.20.3 pyasn1-modules-0.3.0 pytorch-lightning-1.9.0 regex-2023.8.8 requests-oauthlib-1.3.1 safetensors-0.3.1 sentencepiece-0.1.99 tensorboard-2.11.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 timm-0.9.6 tokenizers-0.13.3 toml-0.10.2 toolz-0.12.0 torchmetrics-1.1.1 transformers-4.30.2 voluptuous-0.13.1 yarl-1.9.2\n",
      "Collecting xformers==0.0.20\n",
      "  Obtaining dependency information for xformers==0.0.20 from https://files.pythonhosted.org/packages/4b/b0/dfbb3b0ceafdb73cd1b2bbe33f65dc1c5c47dcb0d4b03ba6f95da6557306/xformers-0.0.20-cp310-cp310-manylinux2014_x86_64.whl.metadata\n",
      "  Downloading xformers-0.0.20-cp310-cp310-manylinux2014_x86_64.whl.metadata (1.1 kB)\n",
      "Collecting wandb\n",
      "  Obtaining dependency information for wandb from https://files.pythonhosted.org/packages/3d/2d/4b115c075a4a5eda4c905efb012a585813d6d40ee3199393bc48f1fdba1f/wandb-0.15.9-py3-none-any.whl.metadata\n",
      "  Downloading wandb-0.15.9-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting prodigyopt\n",
      "  Obtaining dependency information for prodigyopt from https://files.pythonhosted.org/packages/46/43/6c1b6dfaf9a864c0f9c8bad0d20ac6aa2135775c0093fbb4f8bb947edc70/prodigyopt-1.0-py3-none-any.whl.metadata\n",
      "  Downloading prodigyopt-1.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from xformers==0.0.20) (1.24.4)\n",
      "Collecting pyre-extensions==0.0.29 (from xformers==0.0.20)\n",
      "  Downloading pyre_extensions-0.0.29-py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: torch==2.0.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from xformers==0.0.20) (2.0.1+cu118)\n",
      "Collecting typing-inspect (from pyre-extensions==0.0.29->xformers==0.0.20)\n",
      "  Obtaining dependency information for typing-inspect from https://files.pythonhosted.org/packages/65/f3/107a22063bf27bdccf2024833d3445f4eea42b2e598abfbd46f6a63b6cb0/typing_inspect-0.9.0-py3-none-any.whl.metadata\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: typing-extensions in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pyre-extensions==0.0.29->xformers==0.0.20) (4.7.1)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch==2.0.1->xformers==0.0.20) (3.12.2)\n",
      "Requirement already satisfied: sympy in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch==2.0.1->xformers==0.0.20) (1.12)\n",
      "Requirement already satisfied: networkx in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch==2.0.1->xformers==0.0.20) (3.1)\n",
      "Requirement already satisfied: jinja2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch==2.0.1->xformers==0.0.20) (3.1.2)\n",
      "Requirement already satisfied: triton==2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch==2.0.1->xformers==0.0.20) (2.0.0)\n",
      "Requirement already satisfied: cmake in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from triton==2.0.0->torch==2.0.1->xformers==0.0.20) (3.25.0)\n",
      "Requirement already satisfied: lit in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from triton==2.0.0->torch==2.0.1->xformers==0.0.20) (15.0.7)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from wandb) (8.1.6)\n",
      "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb)\n",
      "  Obtaining dependency information for GitPython!=3.1.29,>=1.0.0 from https://files.pythonhosted.org/packages/67/50/742c2fb60989b76ccf7302c7b1d9e26505d7054c24f08cc7ec187faaaea7/GitPython-3.1.32-py3-none-any.whl.metadata\n",
      "  Using cached GitPython-3.1.32-py3-none-any.whl.metadata (10.0 kB)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from wandb) (2.31.0)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from wandb) (5.9.5)\n",
      "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
      "  Obtaining dependency information for sentry-sdk>=1.0.0 from https://files.pythonhosted.org/packages/17/22/dbd5f854f373214d48585eeb6844e50a8dd1600f435d9033493f76f66dfa/sentry_sdk-1.30.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading sentry_sdk-1.30.0-py2.py3-none-any.whl.metadata (9.6 kB)\n",
      "Collecting docker-pycreds>=0.4.0 (from wandb)\n",
      "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Requirement already satisfied: PyYAML in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from wandb) (6.0)\n",
      "Collecting pathtools (from wandb)\n",
      "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting setproctitle (from wandb)\n",
      "  Downloading setproctitle-1.3.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
      "Requirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from wandb) (68.0.0)\n",
      "Collecting appdirs>=1.4.3 (from wandb)\n",
      "  Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from wandb) (3.20.3)\n",
      "Requirement already satisfied: six>=1.4.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb)\n",
      "  Using cached gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2023.5.7)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb)\n",
      "  Using cached smmap-5.0.0-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jinja2->torch==2.0.1->xformers==0.0.20) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sympy->torch==2.0.1->xformers==0.0.20) (1.3.0)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect->pyre-extensions==0.0.29->xformers==0.0.20)\n",
      "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Downloading xformers-0.0.20-cp310-cp310-manylinux2014_x86_64.whl (109.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.1/109.1 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading wandb-0.15.9-py3-none-any.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading prodigyopt-1.0-py3-none-any.whl (5.5 kB)\n",
      "Using cached GitPython-3.1.32-py3-none-any.whl (188 kB)\n",
      "Downloading sentry_sdk-1.30.0-py2.py3-none-any.whl (218 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m218.8/218.8 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Building wheels for collected packages: pathtools\n",
      "  Building wheel for pathtools (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8791 sha256=ffe096f517be7bbcdab93c66ac7a587d09281f54f3e0d2c83d210ccaf0f6cf99\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/e7/f3/22/152153d6eb222ee7a56ff8617d80ee5207207a8c00a7aab794\n",
      "Successfully built pathtools\n",
      "Installing collected packages: pathtools, appdirs, smmap, setproctitle, sentry-sdk, prodigyopt, mypy-extensions, docker-pycreds, typing-inspect, gitdb, pyre-extensions, GitPython, wandb, xformers\n",
      "Successfully installed GitPython-3.1.32 appdirs-1.4.4 docker-pycreds-0.4.0 gitdb-4.0.10 mypy-extensions-1.0.0 pathtools-0.1.2 prodigyopt-1.0 pyre-extensions-0.0.29 sentry-sdk-1.30.0 setproctitle-1.3.2 smmap-5.0.0 typing-inspect-0.9.0 wandb-0.15.9 xformers-0.0.20\n"
     ]
    }
   ],
   "source": [
    "!git clone -b sdxl https://github.com/kohya-ss/sd-scripts\n",
    "%cd /home/ec2-user/SageMaker/sd-scripts/\n",
    "!sed -i 's/diffusers\\[torch\\]==0\\.18\\.2/diffusers[torch]==0.19.3/g; s/tensorboard==2\\.10\\.1/tensorboard==2.11.0/g' requirements.txt\n",
    "!pip install torch==2.0.1+cu118 torchvision==0.15.2+cu118 --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install -U -r requirements.txt\n",
    "!pip install xformers==0.0.20 wandb prodigyopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cebce61-fcef-4b4b-9dfa-ece3e9c8d8c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# fix some warnings and bugs poping up from notebook\n",
    "\n",
    "!sudo rm -rf /lib64/libstdc++.so.6 /lib64/libstdc++.so.6\n",
    "!sudo ln -s /home/ec2-user/anaconda3/envs/python3/lib/libstdc++.so.6 /lib64/libstdc++.so.6\n",
    "\n",
    "import os\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28805cab-2c77-45b7-8670-8073f82385f9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. Initialize training environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b22e7393-be04-425f-b96f-5ae5e2c56ad1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/ec2-user/.cache/huggingface/accelerate/default_config.yaml')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from accelerate.utils import write_basic_config\n",
    "write_basic_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6aec8da5-8da0-481d-8891-88d1af06137f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing dataset.toml\n"
     ]
    }
   ],
   "source": [
    "%%writefile dataset.toml\n",
    "[general]\n",
    "enable_bucket = true\n",
    "shuffle_caption = true\n",
    "caption_extension = '.txt'\n",
    "keep_tokens = 0\n",
    "\n",
    "# DreamBooth caption based character datasets\n",
    "[[datasets]]\n",
    "resolution = 1024\n",
    "# min_bucket_reso = 640\n",
    "# max_bucket_reso = 1536\n",
    "bucket_reso_steps = 32\n",
    "batch_size = 2\n",
    "\n",
    "  [[datasets.subsets]]\n",
    "  image_dir = './images/sample'\n",
    "  num_repeats = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d7153f6-983c-4cbf-ac55-93f65c700cc8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing sample_prompts.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile sample_prompts.txt\n",
    "# prompt 1\n",
    "wta, 1girl, looking_at_viewer, blue_hair, short_twintails, hair_ornament, blue_eyes, blush, smile, open_mouth, shirt, skirt, kneehighs, brown_footwear, standing, solo --n lowres, worst quality, ugly, extra limbs, deformed legs, disfigured legs, (disfigured), ((mutated hands, misshapen hands, mutated fingers, fused fingers):1.2) --w 1024 --h 1024 --d 3129467234 --l 7.5 --s 20\n",
    "\n",
    "# # prompt 2\n",
    "wta, 1girl, looking_at_viewer, blue_hair, short_twintails, hair_ornament, blue_eyes, blush, smile, open_mouth, shirt, skirt, kneehighs, brown_footwear, standing, solo --n lowres, worst quality, ugly, extra limbs, deformed legs, disfigured legs, (disfigured), ((mutated hands, misshapen hands, mutated fingers, fused fingers):1.2) --w 1024 --h 1024 --d 3129467235 --l 7.5 --s 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ea22920c-39b8-4230-8381-190b6c7c45bf",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL_NAME = \"stabilityai/stable-diffusion-xl-base-1.0\"\n",
    "DATASET_CONFIG = \"./dataset.toml\"\n",
    "TRAIN_DATA_DIR = \"./images/sample\"\n",
    "IMAGES_OUTPTS = \"./images/outputs\"\n",
    "LORA_WEIGHT = \"./lora_weight\"\n",
    "!mkdir -p $TRAIN_DATA_DIR $IMAGES_OUTPTS $LORA_WEIGHT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5c66c18c-63a6-4bf7-9af1-a18c020891ae",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'dreambooth-stablediffusion-sagemaker-notebook'...\n",
      "remote: Enumerating objects: 914, done.\u001b[K\n",
      "remote: Counting objects: 100% (19/19), done.\u001b[K\n",
      "remote: Compressing objects: 100% (15/15), done.\u001b[K\n",
      "remote: Total 914 (delta 7), reused 14 (delta 4), pack-reused 895\u001b[K\n",
      "Receiving objects: 100% (914/914), 124.22 MiB | 21.65 MiB/s, done.\n",
      "Resolving deltas: 100% (236/236), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/terrificdm/dreambooth-stablediffusion-sagemaker-notebook\n",
    "!cp -r dreambooth-stablediffusion-sagemaker-notebook/images/Wikipe-tan/* $TRAIN_DATA_DIR\n",
    "!rm -rf dreambooth-stablediffusion-sagemaker-notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed821d3-1c86-4a3e-aa68-6c3e7676bcdf",
   "metadata": {},
   "source": [
    "## 3. Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca27b10-ad2a-4cf0-a194-18c2f63549ae",
   "metadata": {},
   "source": [
    "## LoRA Training with --Multi-gpu option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c3ba10-6d37-4b0f-a502-8fa71b76c5f2",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prepare tokenizers\n",
      "prepare tokenizers\n",
      "prepare tokenizers\n",
      "prepare tokenizers\n",
      "update token length: 225\n",
      "Loading dataset config from dataset.toml\n",
      "update token length: 225\n",
      "Loading dataset config from dataset.toml\n",
      "update token length: 225\n",
      "Loading dataset config from dataset.toml\n",
      "update token length: 225\n",
      "Loading dataset config from dataset.toml\n",
      "prepare images.\n",
      "found directory ./images/sample contains 12 image files\n",
      "120 train images with repeating.\n",
      "0 reg images.\n",
      "no regularization images / 正則化画像が見つかりませんでした\n",
      "[Dataset 0]\n",
      "  batch_size: 2\n",
      "  resolution: (1024, 1024)\n",
      "  enable_bucket: True\n",
      "  min_bucket_reso: 256\n",
      "  max_bucket_reso: 1024\n",
      "  bucket_reso_steps: 32\n",
      "  bucket_no_upscale: False\n",
      "\n",
      "  [Subset 0 of Dataset 0]\n",
      "    image_dir: \"./images/sample\"\n",
      "    image_count: 12\n",
      "    num_repeats: 10\n",
      "    shuffle_caption: True\n",
      "    keep_tokens: 0\n",
      "    caption_dropout_rate: 0.0\n",
      "    caption_dropout_every_n_epoches: 0\n",
      "    caption_tag_dropout_rate: 0.0\n",
      "    color_aug: False\n",
      "    flip_aug: False\n",
      "    face_crop_aug_range: None\n",
      "    random_crop: False\n",
      "    token_warmup_min: 1,\n",
      "    token_warmup_step: 0,\n",
      "    is_reg: False\n",
      "    class_tokens: None\n",
      "    caption_extension: .txt\n",
      "\n",
      "\n",
      "[Dataset 0]\n",
      "loading image sizes.\n",
      "100%|█████████████████████████████████████████| 12/12 [00:00<00:00, 3483.64it/s]\n",
      "make buckets\n",
      "number of images (including repeats) / 各bucketの画像枚数（繰り返し回数を含む）\n",
      "bucket 0: resolution (736, 1024), count: 120\n",
      "mean ar error (without repeats): 0.008522727272727293\n",
      "preparing accelerator\n",
      "prepare images.\n",
      "found directory ./images/sample contains 12 image files\n",
      "120 train images with repeating.\n",
      "0 reg images.\n",
      "no regularization images / 正則化画像が見つかりませんでした\n",
      "[Dataset 0]\n",
      "  batch_size: 2\n",
      "  resolution: (1024, 1024)\n",
      "  enable_bucket: True\n",
      "  min_bucket_reso: 256\n",
      "  max_bucket_reso: 1024\n",
      "  bucket_reso_steps: 32\n",
      "  bucket_no_upscale: False\n",
      "\n",
      "  [Subset 0 of Dataset 0]\n",
      "    image_dir: \"./images/sample\"\n",
      "    image_count: 12\n",
      "    num_repeats: 10\n",
      "    shuffle_caption: True\n",
      "    keep_tokens: 0\n",
      "    caption_dropout_rate: 0.0\n",
      "    caption_dropout_every_n_epoches: 0\n",
      "    caption_tag_dropout_rate: 0.0\n",
      "    color_aug: False\n",
      "    flip_aug: False\n",
      "    face_crop_aug_range: None\n",
      "    random_crop: False\n",
      "    token_warmup_min: 1,\n",
      "    token_warmup_step: 0,\n",
      "    is_reg: False\n",
      "    class_tokens: None\n",
      "    caption_extension: .txt\n",
      "\n",
      "\n",
      "[Dataset 0]\n",
      "loading image sizes.\n",
      "100%|█████████████████████████████████████████| 12/12 [00:00<00:00, 3512.33it/s]\n",
      "make buckets\n",
      "number of images (including repeats) / 各bucketの画像枚数（繰り返し回数を含む）\n",
      "bucket 0: resolution (736, 1024), count: 120\n",
      "mean ar error (without repeats): 0.008522727272727293\n",
      "preparing accelerator\n",
      "prepare images.\n",
      "found directory ./images/sample contains 12 image files\n",
      "120 train images with repeating.\n",
      "0 reg images.\n",
      "no regularization images / 正則化画像が見つかりませんでした\n",
      "[Dataset 0]\n",
      "  batch_size: 2\n",
      "  resolution: (1024, 1024)\n",
      "  enable_bucket: True\n",
      "  min_bucket_reso: 256\n",
      "  max_bucket_reso: 1024\n",
      "  bucket_reso_steps: 32\n",
      "  bucket_no_upscale: False\n",
      "\n",
      "  [Subset 0 of Dataset 0]\n",
      "    image_dir: \"./images/sample\"\n",
      "    image_count: 12\n",
      "    num_repeats: 10\n",
      "    shuffle_caption: True\n",
      "    keep_tokens: 0\n",
      "    caption_dropout_rate: 0.0\n",
      "    caption_dropout_every_n_epoches: 0\n",
      "    caption_tag_dropout_rate: 0.0\n",
      "    color_aug: False\n",
      "    flip_aug: False\n",
      "    face_crop_aug_range: None\n",
      "    random_crop: False\n",
      "    token_warmup_min: 1,\n",
      "    token_warmup_step: 0,\n",
      "    is_reg: False\n",
      "    class_tokens: None\n",
      "    caption_extension: .txt\n",
      "\n",
      "\n",
      "[Dataset 0]\n",
      "loading image sizes.\n",
      "100%|█████████████████████████████████████████| 12/12 [00:00<00:00, 3456.84it/s]\n",
      "make buckets\n",
      "number of images (including repeats) / 各bucketの画像枚数（繰り返し回数を含む）\n",
      "bucket 0: resolution (736, 1024), count: 120\n",
      "mean ar error (without repeats): 0.008522727272727293\n",
      "preparing accelerator\n",
      "prepare images.\n",
      "found directory ./images/sample contains 12 image files\n",
      "120 train images with repeating.\n",
      "0 reg images.\n",
      "no regularization images / 正則化画像が見つかりませんでした\n",
      "[Dataset 0]\n",
      "  batch_size: 2\n",
      "  resolution: (1024, 1024)\n",
      "  enable_bucket: True\n",
      "  min_bucket_reso: 256\n",
      "  max_bucket_reso: 1024\n",
      "  bucket_reso_steps: 32\n",
      "  bucket_no_upscale: False\n",
      "\n",
      "  [Subset 0 of Dataset 0]\n",
      "    image_dir: \"./images/sample\"\n",
      "    image_count: 12\n",
      "    num_repeats: 10\n",
      "    shuffle_caption: True\n",
      "    keep_tokens: 0\n",
      "    caption_dropout_rate: 0.0\n",
      "    caption_dropout_every_n_epoches: 0\n",
      "    caption_tag_dropout_rate: 0.0\n",
      "    color_aug: False\n",
      "    flip_aug: False\n",
      "    face_crop_aug_range: None\n",
      "    random_crop: False\n",
      "    token_warmup_min: 1,\n",
      "    token_warmup_step: 0,\n",
      "    is_reg: False\n",
      "    class_tokens: None\n",
      "    caption_extension: .txt\n",
      "\n",
      "\n",
      "[Dataset 0]\n",
      "loading image sizes.\n",
      "100%|█████████████████████████████████████████| 12/12 [00:00<00:00, 3519.94it/s]\n",
      "make buckets\n",
      "number of images (including repeats) / 各bucketの画像枚数（繰り返し回数を含む）\n",
      "bucket 0: resolution (736, 1024), count: 120\n",
      "mean ar error (without repeats): 0.008522727272727293\n",
      "preparing accelerator\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/ec2-user/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/ec2-user/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/ec2-user/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/ec2-user/.netrc\n",
      "loading model for process 0/4\n",
      "load Diffusers pretrained models: stabilityai/stable-diffusion-xl-base-1.0, variant=None\n",
      "Loading pipeline components...: 100%|█████████████| 6/6 [00:04<00:00,  1.35it/s]\n",
      "U-Net converted to original U-Net\n",
      "loading model for process 1/4\n",
      "load Diffusers pretrained models: stabilityai/stable-diffusion-xl-base-1.0, variant=None\n",
      "Loading pipeline components...: 100%|█████████████| 6/6 [00:04<00:00,  1.35it/s]\n",
      "U-Net converted to original U-Net\n",
      "loading model for process 2/4\n",
      "load Diffusers pretrained models: stabilityai/stable-diffusion-xl-base-1.0, variant=None\n",
      "Loading pipeline components...: 100%|█████████████| 6/6 [00:04<00:00,  1.35it/s]\n",
      "U-Net converted to original U-Net\n",
      "loading model for process 3/4\n",
      "load Diffusers pretrained models: stabilityai/stable-diffusion-xl-base-1.0, variant=None\n",
      "Loading pipeline components...: 100%|█████████████| 6/6 [00:04<00:00,  1.35it/s]\n",
      "U-Net converted to original U-Net\n",
      "Enable xformers for U-NetEnable xformers for U-Net\n",
      "\n",
      "Enable xformers for U-Net\n",
      "Enable xformers for U-Net\n",
      "import network module: networks.lora\n",
      "create LoRA network. base dim (rank): 64, alpha: 32.0\n",
      "neuron dropout: p=None, rank dropout: p=None, module dropout: p=None\n",
      "create LoRA for Text Encoder 1:\n",
      "create LoRA network. base dim (rank): 64, alpha: 32.0\n",
      "neuron dropout: p=None, rank dropout: p=None, module dropout: p=None\n",
      "create LoRA for Text Encoder 1:\n",
      "create LoRA network. base dim (rank): 64, alpha: 32.0\n",
      "neuron dropout: p=None, rank dropout: p=None, module dropout: p=None\n",
      "create LoRA for Text Encoder 1:\n",
      "create LoRA network. base dim (rank): 64, alpha: 32.0\n",
      "neuron dropout: p=None, rank dropout: p=None, module dropout: p=None\n",
      "create LoRA for Text Encoder 1:\n",
      "create LoRA for Text Encoder 2:\n",
      "create LoRA for Text Encoder 2:\n",
      "create LoRA for Text Encoder 2:\n",
      "create LoRA for Text Encoder 2:\n",
      "create LoRA for Text Encoder: 264 modules.\n",
      "create LoRA for Text Encoder: 264 modules.\n",
      "create LoRA for Text Encoder: 264 modules.\n",
      "create LoRA for Text Encoder: 264 modules.\n",
      "create LoRA for U-Net: 722 modules.\n",
      "create LoRA for U-Net: 722 modules.\n",
      "enable LoRA for U-Net\n",
      "enable LoRA for U-Net\n",
      "create LoRA for U-Net: 722 modules.\n",
      "enable LoRA for U-Net\n",
      "create LoRA for U-Net: 722 modules.\n",
      "enable LoRA for U-Net\n",
      "use Prodigy optimizer | {}\n",
      "use Prodigy optimizer | {}\n",
      "prepare optimizer, data loader etc.\n",
      "use Prodigy optimizer | {}\n",
      "override steps. steps for 10 epochs is / 指定エポックまでのステップ数: 150\n",
      "enable full bf16 training.\n",
      "use Prodigy optimizer | {}\n",
      "running training / 学習開始\n",
      "  num train images * repeats / 学習画像の数×繰り返し回数: 120\n",
      "  num reg images / 正則化画像の数: 0\n",
      "  num batches per epoch / 1epochのバッチ数: 15\n",
      "  num epochs / epoch数: 10\n",
      "  batch size per device / バッチサイズ: 2\n",
      "  gradient accumulation steps / 勾配を合計するステップ数 = 1\n",
      "  total optimization steps / 学習ステップ数: 150\n",
      "steps:   0%|                                            | 0/150 [00:00<?, ?it/s]\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mfreddysun\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m./logs/20230828161126/wandb/run-20230828_161200-bay421qb\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mdazzling-surf-15\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/freddysun/lora_wta\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/freddysun/lora_wta/runs/bay421qb\u001b[0m\n",
      "\n",
      "epoch 1/10\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:339: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  and inp.query.storage().data_ptr() == inp.key.storage().data_ptr()\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:339: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  and inp.query.storage().data_ptr() == inp.key.storage().data_ptr()\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:339: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  and inp.query.storage().data_ptr() == inp.key.storage().data_ptr()\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:339: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  and inp.query.storage().data_ptr() == inp.key.storage().data_ptr()\n",
      "steps:  10%|██▏                   | 15/150 [01:26<13:01,  5.79s/it, loss=0.0505]\n",
      "epoch 2/10\n",
      "steps:  20%|████▍                 | 30/150 [01:57<07:48,  3.90s/it, loss=0.0423]\n",
      "generating sample images at step / サンプル画像生成 ステップ: 30\n",
      "generating sample images at step / サンプル画像生成 ステップ: 30\n",
      "\n",
      "generating sample images at step / サンプル画像生成 ステップ: 30\n",
      "\n",
      "\n",
      "saving checkpoint: ./lora_weight/lora_wta-000002.safetensors\n",
      "\n",
      "generating sample images at step / サンプル画像生成 ステップ: 30\n",
      "prompt: wta, 1girl, looking_at_viewer, blue_hair, short_twintails, hair_ornament, blue_eyes, blush, smile, open_mouth, shirt, skirt, kneehighs, brown_footwear, standing, solo\n",
      "negative_prompt: lowres, worst quality, ugly, extra limbs, deformed legs, disfigured legs, (disfigured), ((mutated hands, misshapen hands, mutated fingers, fused fingers):1.2)\n",
      "height: 1024\n",
      "width: 1024\n",
      "sample_steps: 20\n",
      "scale: 7.5\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n",
      "\n",
      "  0%|                                                    | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "  5%|██▏                                         | 1/20 [00:00<00:12,  1.55it/s]\u001b[A\n",
      " 10%|████▍                                       | 2/20 [00:01<00:09,  1.81it/s]\u001b[A\n",
      " 15%|██████▌                                     | 3/20 [00:01<00:08,  1.92it/s]\u001b[A\n",
      " 20%|████████▊                                   | 4/20 [00:02<00:08,  1.97it/s]\u001b[A\n",
      " 25%|███████████                                 | 5/20 [00:02<00:07,  2.00it/s]\u001b[A\n",
      " 30%|█████████████▏                              | 6/20 [00:03<00:06,  2.02it/s]\u001b[A\n",
      " 35%|███████████████▍                            | 7/20 [00:03<00:06,  2.03it/s]\u001b[A\n",
      " 40%|█████████████████▌                          | 8/20 [00:04<00:05,  2.04it/s]\u001b[A\n",
      " 45%|███████████████████▊                        | 9/20 [00:04<00:05,  2.04it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 10/20 [00:05<00:04,  2.05it/s]\u001b[A\n",
      " 55%|███████████████████████▋                   | 11/20 [00:05<00:04,  2.05it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 12/20 [00:05<00:03,  2.05it/s]\u001b[A\n",
      " 65%|███████████████████████████▉               | 13/20 [00:06<00:03,  2.05it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 14/20 [00:06<00:02,  2.05it/s]\u001b[A\n",
      " 75%|████████████████████████████████▎          | 15/20 [00:07<00:02,  2.05it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 16/20 [00:07<00:01,  2.05it/s]\u001b[A\n",
      " 85%|████████████████████████████████████▌      | 17/20 [00:08<00:01,  2.05it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 18/20 [00:08<00:00,  2.05it/s]\u001b[A\n",
      " 95%|████████████████████████████████████████▊  | 19/20 [00:09<00:00,  2.05it/s]\u001b[A\n",
      "100%|███████████████████████████████████████████| 20/20 [00:09<00:00,  2.05it/s]\u001b[A\n",
      "                                                                                \u001b[Aprompt: wta, 1girl, looking_at_viewer, blue_hair, short_twintails, hair_ornament, blue_eyes, blush, smile, open_mouth, shirt, skirt, kneehighs, brown_footwear, standing, solo\n",
      "negative_prompt: lowres, worst quality, ugly, extra limbs, deformed legs, disfigured legs, (disfigured), ((mutated hands, misshapen hands, mutated fingers, fused fingers):1.2)\n",
      "height: 1024\n",
      "width: 1024\n",
      "sample_steps: 20\n",
      "scale: 7.5\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n",
      "\n",
      "  0%|                                                    | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "  5%|██▏                                         | 1/20 [00:00<00:09,  2.06it/s]\u001b[A\n",
      " 10%|████▍                                       | 2/20 [00:00<00:08,  2.06it/s]\u001b[A\n",
      " 15%|██████▌                                     | 3/20 [00:01<00:08,  2.06it/s]\u001b[A\n",
      " 20%|████████▊                                   | 4/20 [00:01<00:07,  2.06it/s]\u001b[A\n",
      " 25%|███████████                                 | 5/20 [00:02<00:07,  2.06it/s]\u001b[A\n",
      " 30%|█████████████▏                              | 6/20 [00:02<00:06,  2.06it/s]\u001b[A\n",
      " 35%|███████████████▍                            | 7/20 [00:03<00:06,  2.06it/s]\u001b[A\n",
      " 40%|█████████████████▌                          | 8/20 [00:03<00:05,  2.06it/s]\u001b[A\n",
      " 45%|███████████████████▊                        | 9/20 [00:04<00:05,  2.06it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 10/20 [00:04<00:04,  2.06it/s]\u001b[A\n",
      " 55%|███████████████████████▋                   | 11/20 [00:05<00:04,  2.06it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 12/20 [00:05<00:03,  2.06it/s]\u001b[A\n",
      " 65%|███████████████████████████▉               | 13/20 [00:06<00:03,  2.06it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 14/20 [00:06<00:02,  2.06it/s]\u001b[A\n",
      " 75%|████████████████████████████████▎          | 15/20 [00:07<00:02,  2.06it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 16/20 [00:07<00:01,  2.06it/s]\u001b[A\n",
      " 85%|████████████████████████████████████▌      | 17/20 [00:08<00:01,  2.06it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 18/20 [00:08<00:00,  2.06it/s]\u001b[A\n",
      " 95%|████████████████████████████████████████▊  | 19/20 [00:09<00:00,  2.06it/s]\u001b[A\n",
      "100%|███████████████████████████████████████████| 20/20 [00:09<00:00,  2.06it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "epoch 3/10\n",
      "steps:  30%|██████▌               | 45/150 [03:01<07:04,  4.04s/it, loss=0.0351]\n",
      "epoch 4/10\n",
      "steps:  40%|████████▊             | 60/150 [03:31<05:17,  3.53s/it, loss=0.0379]\n",
      "generating sample images at step / サンプル画像生成 ステップ: 60\n",
      "generating sample images at step / サンプル画像生成 ステップ: 60\n",
      "generating sample images at step / サンプル画像生成 ステップ: 60\n",
      "\n",
      "\n",
      "\n",
      "saving checkpoint: ./lora_weight/lora_wta-000004.safetensors\n",
      "\n",
      "generating sample images at step / サンプル画像生成 ステップ: 60\n",
      "prompt: wta, 1girl, looking_at_viewer, blue_hair, short_twintails, hair_ornament, blue_eyes, blush, smile, open_mouth, shirt, skirt, kneehighs, brown_footwear, standing, solo\n",
      "negative_prompt: lowres, worst quality, ugly, extra limbs, deformed legs, disfigured legs, (disfigured), ((mutated hands, misshapen hands, mutated fingers, fused fingers):1.2)\n",
      "height: 1024\n",
      "width: 1024\n",
      "sample_steps: 20\n",
      "scale: 7.5\n",
      "\n",
      "  0%|                                                    | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "  5%|██▏                                         | 1/20 [00:00<00:09,  2.06it/s]\u001b[A\n",
      " 10%|████▍                                       | 2/20 [00:00<00:08,  2.06it/s]\u001b[A\n",
      " 15%|██████▌                                     | 3/20 [00:01<00:08,  2.06it/s]\u001b[A\n",
      " 20%|████████▊                                   | 4/20 [00:01<00:07,  2.06it/s]\u001b[A\n",
      " 25%|███████████                                 | 5/20 [00:02<00:07,  2.06it/s]\u001b[A\n",
      " 30%|█████████████▏                              | 6/20 [00:02<00:06,  2.06it/s]\u001b[A\n",
      " 35%|███████████████▍                            | 7/20 [00:03<00:06,  2.06it/s]\u001b[A\n",
      " 40%|█████████████████▌                          | 8/20 [00:03<00:05,  2.06it/s]\u001b[A\n",
      " 45%|███████████████████▊                        | 9/20 [00:04<00:05,  2.06it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 10/20 [00:04<00:04,  2.06it/s]\u001b[A\n",
      " 55%|███████████████████████▋                   | 11/20 [00:05<00:04,  2.06it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 12/20 [00:05<00:03,  2.06it/s]\u001b[A\n",
      " 65%|███████████████████████████▉               | 13/20 [00:06<00:03,  2.06it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 14/20 [00:06<00:02,  2.06it/s]\u001b[A\n",
      " 75%|████████████████████████████████▎          | 15/20 [00:07<00:02,  2.06it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 16/20 [00:07<00:01,  2.06it/s]\u001b[A\n",
      " 85%|████████████████████████████████████▌      | 17/20 [00:08<00:01,  2.06it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 18/20 [00:08<00:00,  2.06it/s]\u001b[A\n",
      " 95%|████████████████████████████████████████▊  | 19/20 [00:09<00:00,  2.06it/s]\u001b[A\n",
      "100%|███████████████████████████████████████████| 20/20 [00:09<00:00,  2.06it/s]\u001b[A\n",
      "                                                                                \u001b[Aprompt: wta, 1girl, looking_at_viewer, blue_hair, short_twintails, hair_ornament, blue_eyes, blush, smile, open_mouth, shirt, skirt, kneehighs, brown_footwear, standing, solo\n",
      "negative_prompt: lowres, worst quality, ugly, extra limbs, deformed legs, disfigured legs, (disfigured), ((mutated hands, misshapen hands, mutated fingers, fused fingers):1.2)\n",
      "height: 1024\n",
      "width: 1024\n",
      "sample_steps: 20\n",
      "scale: 7.5\n",
      "\n",
      "  0%|                                                    | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "  5%|██▏                                         | 1/20 [00:00<00:09,  2.06it/s]\u001b[A\n",
      " 10%|████▍                                       | 2/20 [00:00<00:08,  2.06it/s]\u001b[A\n",
      " 15%|██████▌                                     | 3/20 [00:01<00:08,  2.06it/s]\u001b[A\n",
      " 20%|████████▊                                   | 4/20 [00:01<00:07,  2.06it/s]\u001b[A\n",
      " 25%|███████████                                 | 5/20 [00:02<00:07,  2.06it/s]\u001b[A\n",
      " 30%|█████████████▏                              | 6/20 [00:02<00:06,  2.06it/s]\u001b[A\n",
      " 35%|███████████████▍                            | 7/20 [00:03<00:06,  2.06it/s]\u001b[A\n",
      " 40%|█████████████████▌                          | 8/20 [00:03<00:05,  2.06it/s]\u001b[A\n",
      " 45%|███████████████████▊                        | 9/20 [00:04<00:05,  2.06it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 10/20 [00:04<00:04,  2.06it/s]\u001b[A\n",
      " 55%|███████████████████████▋                   | 11/20 [00:05<00:04,  2.06it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 12/20 [00:05<00:03,  2.06it/s]\u001b[A\n",
      " 65%|███████████████████████████▉               | 13/20 [00:06<00:03,  2.06it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 14/20 [00:06<00:02,  2.06it/s]\u001b[A\n",
      " 75%|████████████████████████████████▎          | 15/20 [00:07<00:02,  2.06it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 16/20 [00:07<00:01,  2.06it/s]\u001b[A\n",
      " 85%|████████████████████████████████████▌      | 17/20 [00:08<00:01,  2.06it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 18/20 [00:08<00:00,  2.06it/s]\u001b[A\n",
      " 95%|████████████████████████████████████████▊  | 19/20 [00:09<00:00,  2.06it/s]\u001b[A\n",
      "100%|███████████████████████████████████████████| 20/20 [00:09<00:00,  2.06it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "epoch 5/10\n",
      "steps:  50%|███████████           | 75/150 [04:26<04:26,  3.55s/it, loss=0.0499]\n",
      "epoch 6/10\n",
      "steps:  60%|█████████████▏        | 90/150 [04:56<03:17,  3.29s/it, loss=0.0377]\n",
      "generating sample images at step / サンプル画像生成 ステップ: 90\n",
      "generating sample images at step / サンプル画像生成 ステップ: 90\n",
      "generating sample images at step / サンプル画像生成 ステップ: 90\n",
      "\n",
      "\n",
      "\n",
      "saving checkpoint: ./lora_weight/lora_wta-000006.safetensors\n",
      "\n",
      "generating sample images at step / サンプル画像生成 ステップ: 90\n",
      "prompt: wta, 1girl, looking_at_viewer, blue_hair, short_twintails, hair_ornament, blue_eyes, blush, smile, open_mouth, shirt, skirt, kneehighs, brown_footwear, standing, solo\n",
      "negative_prompt: lowres, worst quality, ugly, extra limbs, deformed legs, disfigured legs, (disfigured), ((mutated hands, misshapen hands, mutated fingers, fused fingers):1.2)\n",
      "height: 1024\n",
      "width: 1024\n",
      "sample_steps: 20\n",
      "scale: 7.5\n",
      "\n",
      "  0%|                                                    | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "  5%|██▏                                         | 1/20 [00:00<00:09,  2.06it/s]\u001b[A\n",
      " 10%|████▍                                       | 2/20 [00:00<00:08,  2.06it/s]\u001b[A\n",
      " 15%|██████▌                                     | 3/20 [00:01<00:08,  2.06it/s]\u001b[A\n",
      " 20%|████████▊                                   | 4/20 [00:01<00:07,  2.06it/s]\u001b[A\n",
      " 25%|███████████                                 | 5/20 [00:02<00:07,  2.06it/s]\u001b[A\n",
      " 30%|█████████████▏                              | 6/20 [00:02<00:06,  2.06it/s]\u001b[A\n",
      " 35%|███████████████▍                            | 7/20 [00:03<00:06,  2.06it/s]\u001b[A\n",
      " 40%|█████████████████▌                          | 8/20 [00:03<00:05,  2.06it/s]\u001b[A\n",
      " 45%|███████████████████▊                        | 9/20 [00:04<00:05,  2.06it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 10/20 [00:04<00:04,  2.06it/s]\u001b[A\n",
      " 55%|███████████████████████▋                   | 11/20 [00:05<00:04,  2.06it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 12/20 [00:05<00:03,  2.06it/s]\u001b[A\n",
      " 65%|███████████████████████████▉               | 13/20 [00:06<00:03,  2.06it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 14/20 [00:06<00:02,  2.06it/s]\u001b[A\n",
      " 75%|████████████████████████████████▎          | 15/20 [00:07<00:02,  2.06it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 16/20 [00:07<00:01,  2.06it/s]\u001b[A\n",
      " 85%|████████████████████████████████████▌      | 17/20 [00:08<00:01,  2.06it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 18/20 [00:08<00:00,  2.06it/s]\u001b[A\n",
      " 95%|████████████████████████████████████████▊  | 19/20 [00:09<00:00,  2.06it/s]\u001b[A\n",
      "100%|███████████████████████████████████████████| 20/20 [00:09<00:00,  2.06it/s]\u001b[A\n",
      "                                                                                \u001b[Aprompt: wta, 1girl, looking_at_viewer, blue_hair, short_twintails, hair_ornament, blue_eyes, blush, smile, open_mouth, shirt, skirt, kneehighs, brown_footwear, standing, solo\n",
      "negative_prompt: lowres, worst quality, ugly, extra limbs, deformed legs, disfigured legs, (disfigured), ((mutated hands, misshapen hands, mutated fingers, fused fingers):1.2)\n",
      "height: 1024\n",
      "width: 1024\n",
      "sample_steps: 20\n",
      "scale: 7.5\n",
      "\n",
      "  0%|                                                    | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "  5%|██▏                                         | 1/20 [00:00<00:09,  2.06it/s]\u001b[A\n",
      " 10%|████▍                                       | 2/20 [00:00<00:08,  2.06it/s]\u001b[A\n",
      " 15%|██████▌                                     | 3/20 [00:01<00:08,  2.06it/s]\u001b[A\n",
      " 20%|████████▊                                   | 4/20 [00:01<00:07,  2.06it/s]\u001b[A\n",
      " 25%|███████████                                 | 5/20 [00:02<00:07,  2.06it/s]\u001b[A\n",
      " 30%|█████████████▏                              | 6/20 [00:02<00:06,  2.06it/s]\u001b[A\n",
      " 35%|███████████████▍                            | 7/20 [00:03<00:06,  2.06it/s]\u001b[A\n",
      " 40%|█████████████████▌                          | 8/20 [00:03<00:05,  2.06it/s]\u001b[A\n",
      " 45%|███████████████████▊                        | 9/20 [00:04<00:05,  2.06it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 10/20 [00:04<00:04,  2.06it/s]\u001b[A\n",
      " 55%|███████████████████████▋                   | 11/20 [00:05<00:04,  2.06it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 12/20 [00:05<00:03,  2.06it/s]\u001b[A\n",
      " 65%|███████████████████████████▉               | 13/20 [00:06<00:03,  2.06it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 14/20 [00:06<00:02,  2.06it/s]\u001b[A\n",
      " 75%|████████████████████████████████▎          | 15/20 [00:07<00:02,  2.06it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 16/20 [00:07<00:01,  2.06it/s]\u001b[A\n",
      " 85%|████████████████████████████████████▌      | 17/20 [00:08<00:01,  2.06it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 18/20 [00:08<00:00,  2.06it/s]\u001b[A\n",
      " 95%|████████████████████████████████████████▊  | 19/20 [00:09<00:00,  2.06it/s]\u001b[A\n",
      "100%|███████████████████████████████████████████| 20/20 [00:09<00:00,  2.06it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "epoch 7/10\n",
      "steps:  70%|██████████████▋      | 105/150 [05:49<02:29,  3.33s/it, loss=0.0409]\n",
      "epoch 8/10\n",
      "steps:  80%|█████████████████▌    | 120/150 [06:20<01:35,  3.17s/it, loss=0.043]\n",
      "generating sample images at step / サンプル画像生成 ステップ: 120\n",
      "generating sample images at step / サンプル画像生成 ステップ: 120\n",
      "generating sample images at step / サンプル画像生成 ステップ: 120\n",
      "\n",
      "\n",
      "\n",
      "saving checkpoint: ./lora_weight/lora_wta-000008.safetensors\n",
      "\n",
      "generating sample images at step / サンプル画像生成 ステップ: 120\n",
      "prompt: wta, 1girl, looking_at_viewer, blue_hair, short_twintails, hair_ornament, blue_eyes, blush, smile, open_mouth, shirt, skirt, kneehighs, brown_footwear, standing, solo\n",
      "negative_prompt: lowres, worst quality, ugly, extra limbs, deformed legs, disfigured legs, (disfigured), ((mutated hands, misshapen hands, mutated fingers, fused fingers):1.2)\n",
      "height: 1024\n",
      "width: 1024\n",
      "sample_steps: 20\n",
      "scale: 7.5\n",
      "\n",
      "  0%|                                                    | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "  5%|██▏                                         | 1/20 [00:00<00:09,  2.06it/s]\u001b[A\n",
      " 10%|████▍                                       | 2/20 [00:00<00:08,  2.06it/s]\u001b[A\n",
      " 15%|██████▌                                     | 3/20 [00:01<00:08,  2.06it/s]\u001b[A\n",
      " 20%|████████▊                                   | 4/20 [00:01<00:07,  2.06it/s]\u001b[A\n",
      " 25%|███████████                                 | 5/20 [00:02<00:07,  2.06it/s]\u001b[A\n",
      " 30%|█████████████▏                              | 6/20 [00:02<00:06,  2.06it/s]\u001b[A\n",
      " 35%|███████████████▍                            | 7/20 [00:03<00:06,  2.06it/s]\u001b[A\n",
      " 40%|█████████████████▌                          | 8/20 [00:03<00:05,  2.06it/s]\u001b[A\n",
      " 45%|███████████████████▊                        | 9/20 [00:04<00:05,  2.06it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 10/20 [00:04<00:04,  2.06it/s]\u001b[A\n",
      " 55%|███████████████████████▋                   | 11/20 [00:05<00:04,  2.06it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 12/20 [00:05<00:03,  2.06it/s]\u001b[A\n",
      " 65%|███████████████████████████▉               | 13/20 [00:06<00:03,  2.06it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 14/20 [00:06<00:02,  2.06it/s]\u001b[A\n",
      " 75%|████████████████████████████████▎          | 15/20 [00:07<00:02,  2.06it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 16/20 [00:07<00:01,  2.06it/s]\u001b[A\n",
      " 85%|████████████████████████████████████▌      | 17/20 [00:08<00:01,  2.06it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 18/20 [00:08<00:00,  2.06it/s]\u001b[A\n",
      " 95%|████████████████████████████████████████▊  | 19/20 [00:09<00:00,  2.06it/s]\u001b[A\n",
      "100%|███████████████████████████████████████████| 20/20 [00:09<00:00,  2.06it/s]\u001b[A\n",
      "                                                                                \u001b[Aprompt: wta, 1girl, looking_at_viewer, blue_hair, short_twintails, hair_ornament, blue_eyes, blush, smile, open_mouth, shirt, skirt, kneehighs, brown_footwear, standing, solo\n",
      "negative_prompt: lowres, worst quality, ugly, extra limbs, deformed legs, disfigured legs, (disfigured), ((mutated hands, misshapen hands, mutated fingers, fused fingers):1.2)\n",
      "height: 1024\n",
      "width: 1024\n",
      "sample_steps: 20\n",
      "scale: 7.5\n",
      "\n",
      "  0%|                                                    | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "  5%|██▏                                         | 1/20 [00:00<00:09,  2.06it/s]\u001b[A\n",
      " 10%|████▍                                       | 2/20 [00:00<00:08,  2.06it/s]\u001b[A\n",
      " 15%|██████▌                                     | 3/20 [00:01<00:08,  2.06it/s]\u001b[A\n",
      " 20%|████████▊                                   | 4/20 [00:01<00:07,  2.06it/s]\u001b[A\n",
      " 25%|███████████                                 | 5/20 [00:02<00:07,  2.06it/s]\u001b[A\n",
      " 30%|█████████████▏                              | 6/20 [00:02<00:06,  2.05it/s]\u001b[A\n",
      " 35%|███████████████▍                            | 7/20 [00:03<00:06,  2.05it/s]\u001b[A\n",
      " 40%|█████████████████▌                          | 8/20 [00:03<00:05,  2.05it/s]\u001b[A\n",
      " 45%|███████████████████▊                        | 9/20 [00:04<00:05,  2.05it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 10/20 [00:04<00:04,  2.05it/s]\u001b[A\n",
      " 55%|███████████████████████▋                   | 11/20 [00:05<00:04,  2.05it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 12/20 [00:05<00:03,  2.05it/s]\u001b[A\n",
      " 65%|███████████████████████████▉               | 13/20 [00:06<00:03,  2.05it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 14/20 [00:06<00:02,  2.05it/s]\u001b[A\n",
      " 75%|████████████████████████████████▎          | 15/20 [00:07<00:02,  2.05it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 16/20 [00:07<00:01,  2.05it/s]\u001b[A\n",
      " 85%|████████████████████████████████████▌      | 17/20 [00:08<00:01,  2.05it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 18/20 [00:08<00:00,  2.05it/s]\u001b[A\n",
      " 95%|████████████████████████████████████████▊  | 19/20 [00:09<00:00,  2.05it/s]\u001b[A\n",
      "100%|███████████████████████████████████████████| 20/20 [00:09<00:00,  2.05it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "epoch 9/10\n",
      "steps:  87%|██████████████████▎  | 131/150 [07:05<01:01,  3.25s/it, loss=0.0416]"
     ]
    }
   ],
   "source": [
    "!accelerate launch --multi_gpu sdxl_train_network.py \\\n",
    "   --pretrained_model_name_or_path=$MODEL_NAME \\\n",
    "   --dataset_config=$DATASET_CONFIG \\\n",
    "   --output_dir=$LORA_WEIGHT \\\n",
    "   --network_module=\"networks.lora\" \\\n",
    "   --max_train_epochs=10 \\\n",
    "   --network_train_unet_only \\\n",
    "   --learning_rate=1.0 \\\n",
    "   --lr_scheduler=\"cosine_with_restarts\" \\\n",
    "   --lr_scheduler_num_cycles=1 \\\n",
    "   --network_dim=64 \\\n",
    "   --network_alpha=32 \\\n",
    "   --output_name=\"lora_wta\" \\\n",
    "   --save_every_n_epochs=2 \\\n",
    "   --mixed_precision=\"bf16\" \\\n",
    "   --full_bf16 \\\n",
    "   --gradient_checkpointing \\\n",
    "   --max_token_length=225 \\\n",
    "   --save_model_as=\"safetensors\" \\\n",
    "   --no_half_vae \\\n",
    "   --xformers \\\n",
    "   --optimizer_type=\"prodigy\" \\\n",
    "   --min_snr_gamma=5 \\\n",
    "   --sample_every_n_epochs=2 \\\n",
    "   --sample_prompts=\"./sample_prompts.txt\" \\\n",
    "   --sample_sampler=\"euler_a\" \\\n",
    "   --logging_dir=\"./logs\" \\\n",
    "   --log_with=\"all\" \\\n",
    "   --log_tracker_name=\"lora_wta\" \\\n",
    "   --wandb_api_key=\"245e51c77454f92bbb80ff882df76d02cfd7f4a5\" # register a user via https://wandb.ai and get an API key \n",
    "   # --cache_text_encoder_outputs_to_disk \\\n",
    "   # --save_model_as=\"safetensors\" \\"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d439701b-773c-42f1-b37e-b13ceb9a71ce",
   "metadata": {},
   "source": [
    "## 3. Use AA pic and caption to training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6ce1871-a197-4702-ae4a-08412df6fb3e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/sd-scripts\n"
     ]
    }
   ],
   "source": [
    "%cd /home/ec2-user/SageMaker/sd-scripts/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6ab7cf6b-e5a8-4e37-b3ef-739e391198fb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting aa/dataset.toml\n"
     ]
    }
   ],
   "source": [
    "%%writefile aa/dataset.toml\n",
    "[general]\n",
    "  enable_bucket = true\n",
    "  shuffle_caption = true\n",
    "  caption_extension = \".txt\"\n",
    "  keep_tokens = 0\n",
    "\n",
    "[[datasets]]\n",
    "  resolution = 1024\n",
    "  batch_size = 1\n",
    "\n",
    "  [[datasets.subsets]]\n",
    "    image_dir = \"./aa/images/sample\"\n",
    "    num_repeats = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ffe989e-64ef-43ff-8efd-3479b74d2a9f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL_NAME = \"stabilityai/stable-diffusion-xl-base-1.0\"\n",
    "DATASET_CONFIG = \"./aa/dataset.toml\"\n",
    "TRAIN_DATA_DIR = \"./aa/images/sample\"\n",
    "IMAGES_OUTPTS = \"./aa/images/outputs\"\n",
    "LORA_WEIGHT = \"./aa/lora_weight\"\n",
    "!mkdir -p $TRAIN_DATA_DIR $IMAGES_OUTPTS $LORA_WEIGHT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7809d6fb-ea45-4434-93de-c0fb8b24f8f8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prepare tokenizersprepare tokenizersprepare tokenizersprepare tokenizers\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)olve/main/vocab.json: 100%|███| 961k/961k [00:00<00:00, 4.74MB/s]\n",
      "Downloading (…)olve/main/merges.txt: 100%|███| 525k/525k [00:00<00:00, 3.79MB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|█████| 389/389 [00:00<00:00, 4.78MB/s]\n",
      "Downloading (…)okenizer_config.json: 100%|█████| 905/905 [00:00<00:00, 10.8MB/s]\n",
      "Downloading (…)olve/main/vocab.json: 100%|███| 862k/862k [00:00<00:00, 12.4MB/s]\n",
      "Downloading (…)olve/main/merges.txt: 100%|███| 525k/525k [00:00<00:00, 94.7MB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|█████| 389/389 [00:00<00:00, 4.76MB/s]\n",
      "Downloading (…)okenizer_config.json: 100%|█████| 904/904 [00:00<00:00, 11.5MB/s]\n",
      "Loading dataset config from aa/dataset.toml\n",
      "Loading dataset config from aa/dataset.toml\n",
      "Loading dataset config from aa/dataset.toml\n",
      "Loading dataset config from aa/dataset.toml\n",
      "prepare images.\n",
      "found directory ./aa/images/sample contains 9 image files\n",
      "180 train images with repeating.\n",
      "0 reg images.\n",
      "no regularization images / 正則化画像が見つかりませんでした\n",
      "[Dataset 0]\n",
      "  batch_size: 1\n",
      "  resolution: (1024, 1024)\n",
      "  enable_bucket: True\n",
      "  min_bucket_reso: 256\n",
      "  max_bucket_reso: 1024\n",
      "  bucket_reso_steps: 64\n",
      "  bucket_no_upscale: False\n",
      "\n",
      "  [Subset 0 of Dataset 0]\n",
      "    image_dir: \"./aa/images/sample\"\n",
      "    image_count: 9\n",
      "    num_repeats: 20\n",
      "    shuffle_caption: True\n",
      "    keep_tokens: 0\n",
      "    caption_dropout_rate: 0.0\n",
      "    caption_dropout_every_n_epoches: 0\n",
      "    caption_tag_dropout_rate: 0.0\n",
      "    color_aug: False\n",
      "    flip_aug: False\n",
      "    face_crop_aug_range: None\n",
      "    random_crop: False\n",
      "    token_warmup_min: 1,\n",
      "    token_warmup_step: 0,\n",
      "    is_reg: False\n",
      "    class_tokens: None\n",
      "    caption_extension: .txt\n",
      "\n",
      "\n",
      "[Dataset 0]\n",
      "loading image sizes.\n",
      "100%|████████████████████████████████████████████| 9/9 [00:00<00:00, 738.39it/s]\n",
      "make buckets\n",
      "number of images (including repeats) / 各bucketの画像枚数（繰り返し回数を含む）\n",
      "bucket 0: resolution (768, 1024), count: 80\n",
      "bucket 1: resolution (832, 1024), count: 20\n",
      "bucket 2: resolution (896, 1024), count: 20\n",
      "bucket 3: resolution (1024, 1024), count: 60\n",
      "mean ar error (without repeats): 0.01191137274693911\n",
      "preparing accelerator\n",
      "prepare images.\n",
      "found directory ./aa/images/sample contains 9 image files\n",
      "180 train images with repeating.\n",
      "0 reg images.\n",
      "no regularization images / 正則化画像が見つかりませんでした\n",
      "[Dataset 0]\n",
      "  batch_size: 1\n",
      "  resolution: (1024, 1024)\n",
      "  enable_bucket: True\n",
      "  min_bucket_reso: 256\n",
      "  max_bucket_reso: 1024\n",
      "  bucket_reso_steps: 64\n",
      "  bucket_no_upscale: False\n",
      "\n",
      "  [Subset 0 of Dataset 0]\n",
      "    image_dir: \"./aa/images/sample\"\n",
      "    image_count: 9\n",
      "    num_repeats: 20\n",
      "    shuffle_caption: True\n",
      "    keep_tokens: 0\n",
      "    caption_dropout_rate: 0.0\n",
      "    caption_dropout_every_n_epoches: 0\n",
      "    caption_tag_dropout_rate: 0.0\n",
      "    color_aug: False\n",
      "    flip_aug: False\n",
      "    face_crop_aug_range: None\n",
      "    random_crop: False\n",
      "    token_warmup_min: 1,\n",
      "    token_warmup_step: 0,\n",
      "    is_reg: False\n",
      "    class_tokens: None\n",
      "    caption_extension: .txt\n",
      "\n",
      "\n",
      "[Dataset 0]\n",
      "loading image sizes.\n",
      "100%|███████████████████████████████████████████| 9/9 [00:00<00:00, 2828.68it/s]\n",
      "make buckets\n",
      "number of images (including repeats) / 各bucketの画像枚数（繰り返し回数を含む）\n",
      "bucket 0: resolution (768, 1024), count: 80\n",
      "bucket 1: resolution (832, 1024), count: 20\n",
      "bucket 2: resolution (896, 1024), count: 20\n",
      "bucket 3: resolution (1024, 1024), count: 60\n",
      "mean ar error (without repeats): 0.01191137274693911\n",
      "preparing accelerator\n",
      "prepare images.\n",
      "found directory ./aa/images/sample contains 9 image files\n",
      "180 train images with repeating.\n",
      "0 reg images.\n",
      "no regularization images / 正則化画像が見つかりませんでした\n",
      "[Dataset 0]\n",
      "  batch_size: 1\n",
      "  resolution: (1024, 1024)\n",
      "  enable_bucket: True\n",
      "  min_bucket_reso: 256\n",
      "  max_bucket_reso: 1024\n",
      "  bucket_reso_steps: 64\n",
      "  bucket_no_upscale: False\n",
      "\n",
      "  [Subset 0 of Dataset 0]\n",
      "    image_dir: \"./aa/images/sample\"\n",
      "    image_count: 9\n",
      "    num_repeats: 20\n",
      "    shuffle_caption: True\n",
      "    keep_tokens: 0\n",
      "    caption_dropout_rate: 0.0\n",
      "    caption_dropout_every_n_epoches: 0\n",
      "    caption_tag_dropout_rate: 0.0\n",
      "    color_aug: False\n",
      "    flip_aug: False\n",
      "    face_crop_aug_range: None\n",
      "    random_crop: False\n",
      "    token_warmup_min: 1,\n",
      "    token_warmup_step: 0,\n",
      "    is_reg: False\n",
      "    class_tokens: None\n",
      "    caption_extension: .txt\n",
      "\n",
      "\n",
      "[Dataset 0]\n",
      "loading image sizes.\n",
      "100%|███████████████████████████████████████████| 9/9 [00:00<00:00, 2803.89it/s]\n",
      "make buckets\n",
      "number of images (including repeats) / 各bucketの画像枚数（繰り返し回数を含む）\n",
      "bucket 0: resolution (768, 1024), count: 80\n",
      "bucket 1: resolution (832, 1024), count: 20\n",
      "bucket 2: resolution (896, 1024), count: 20\n",
      "bucket 3: resolution (1024, 1024), count: 60\n",
      "mean ar error (without repeats): 0.01191137274693911\n",
      "preparing accelerator\n",
      "prepare images.\n",
      "found directory ./aa/images/sample contains 9 image files\n",
      "180 train images with repeating.\n",
      "0 reg images.\n",
      "no regularization images / 正則化画像が見つかりませんでした\n",
      "[Dataset 0]\n",
      "  batch_size: 1\n",
      "  resolution: (1024, 1024)\n",
      "  enable_bucket: True\n",
      "  min_bucket_reso: 256\n",
      "  max_bucket_reso: 1024\n",
      "  bucket_reso_steps: 64\n",
      "  bucket_no_upscale: False\n",
      "\n",
      "  [Subset 0 of Dataset 0]\n",
      "    image_dir: \"./aa/images/sample\"\n",
      "    image_count: 9\n",
      "    num_repeats: 20\n",
      "    shuffle_caption: True\n",
      "    keep_tokens: 0\n",
      "    caption_dropout_rate: 0.0\n",
      "    caption_dropout_every_n_epoches: 0\n",
      "    caption_tag_dropout_rate: 0.0\n",
      "    color_aug: False\n",
      "    flip_aug: False\n",
      "    face_crop_aug_range: None\n",
      "    random_crop: False\n",
      "    token_warmup_min: 1,\n",
      "    token_warmup_step: 0,\n",
      "    is_reg: False\n",
      "    class_tokens: None\n",
      "    caption_extension: .txt\n",
      "\n",
      "\n",
      "[Dataset 0]\n",
      "loading image sizes.\n",
      "100%|███████████████████████████████████████████| 9/9 [00:00<00:00, 2805.14it/s]\n",
      "make buckets\n",
      "number of images (including repeats) / 各bucketの画像枚数（繰り返し回数を含む）\n",
      "bucket 0: resolution (768, 1024), count: 80\n",
      "bucket 1: resolution (832, 1024), count: 20\n",
      "bucket 2: resolution (896, 1024), count: 20\n",
      "bucket 3: resolution (1024, 1024), count: 60\n",
      "mean ar error (without repeats): 0.01191137274693911\n",
      "preparing accelerator\n",
      "loading model for process 0/4\n",
      "load Diffusers pretrained models: stabilityai/stable-diffusion-xl-base-1.0, variant=None\n",
      "Downloading (…)ain/model_index.json: 100%|█████| 609/609 [00:00<00:00, 6.65MB/s]\n",
      "Fetching 15 files:   0%|                                 | 0/15 [00:00<?, ?it/s]\n",
      "Downloading model.safetensors:   0%|                | 0.00/2.78G [00:00<?, ?B/s]\u001b[A\n",
      "\n",
      "Downloading (…)_encoder/config.json: 100%|█████| 565/565 [00:00<00:00, 6.37MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading (…)cheduler_config.json: 100%|█████| 479/479 [00:00<00:00, 3.02MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading (…)cial_tokens_map.json: 100%|█████| 460/460 [00:00<00:00, 4.57MB/s]\u001b[A\u001b[A\n",
      "Fetching 15 files:  13%|███▎                     | 2/15 [00:00<00:01,  9.57it/s]\n",
      "\n",
      "Downloading (…)kenizer_2/merges.txt:   0%|           | 0.00/525k [00:00<?, ?B/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading (…)okenizer_config.json: 100%|█████| 725/725 [00:00<00:00, 6.50MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Downloading model.safetensors:   0%|                 | 0.00/492M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
      "Downloading model.safetensors:   1%|        | 31.5M/2.78G [00:00<00:10, 257MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)ncoder_2/config.json: 100%|█████| 575/575 [00:00<00:00, 4.97MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)ch_model.safetensors:   0%|          | 0.00/10.3G [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Downloading (…)kenizer_2/merges.txt: 100%|███| 525k/525k [00:00<00:00, 3.72MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading (…)1d8/unet/config.json: 100%|█| 1.68k/1.68k [00:00<00:00, 11.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading (…)kenizer_2/vocab.json:   0%|          | 0.00/1.06M [00:00<?, ?B/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)81d8/vae/config.json: 100%|█████| 642/642 [00:00<00:00, 1.59MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Downloading model.safetensors:   6%|▌        | 31.5M/492M [00:00<00:01, 266MB/s]\u001b[A\u001b[A\u001b[A\n",
      "Downloading model.safetensors:   3%|▏       | 73.4M/2.78G [00:00<00:08, 324MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)ch_model.safetensors:   0%|           | 0.00/335M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)ch_model.safetensors:   0%|  | 31.5M/10.3G [00:00<00:41, 247MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)ch_model.safetensors:   0%|           | 0.00/335M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading model.safetensors:  15%|█▎       | 73.4M/492M [00:00<00:01, 267MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Downloading (…)kenizer_2/vocab.json: 100%|█| 1.06M/1.06M [00:00<00:00, 5.26MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)ch_model.safetensors:   6%|▏  | 21.0M/335M [00:00<00:02, 142MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Downloading model.safetensors:   4%|▎        | 115M/2.78G [00:00<00:10, 260MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)ch_model.safetensors:   1%|  | 62.9M/10.3G [00:00<00:48, 212MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)ch_model.safetensors:   6%|▏  | 21.0M/335M [00:00<00:02, 139MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  13%|▍  | 41.9M/335M [00:00<00:01, 160MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  13%|▍  | 41.9M/335M [00:00<00:01, 155MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading model.safetensors:  21%|██▏       | 105M/492M [00:00<00:02, 193MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)ch_model.safetensors:   1%|  | 94.4M/10.3G [00:00<00:54, 187MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Downloading model.safetensors:   5%|▍        | 147M/2.78G [00:00<00:12, 210MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  19%|▌  | 62.9M/335M [00:00<00:01, 161MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  19%|▌  | 62.9M/335M [00:00<00:01, 159MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading model.safetensors:  26%|██▌       | 126M/492M [00:00<00:02, 181MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)ch_model.safetensors:   1%|   | 115M/10.3G [00:00<00:55, 184MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  25%|▊  | 83.9M/335M [00:00<00:01, 169MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Downloading model.safetensors:   6%|▌        | 178M/2.78G [00:00<00:13, 195MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  25%|▊  | 83.9M/335M [00:00<00:01, 163MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading model.safetensors:  30%|██▉       | 147M/492M [00:00<00:01, 178MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)ch_model.safetensors:   1%|   | 136M/10.3G [00:00<00:55, 184MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  31%|█▎  | 105M/335M [00:00<00:01, 173MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Downloading model.safetensors:   7%|▋        | 199M/2.78G [00:00<00:13, 193MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  31%|█▎  | 105M/335M [00:00<00:01, 168MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)ch_model.safetensors:   2%|   | 157M/10.3G [00:00<00:55, 182MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading model.safetensors:  34%|███▍      | 168M/492M [00:00<00:01, 173MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  38%|█▌  | 126M/335M [00:00<00:01, 175MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Downloading model.safetensors:   8%|▋        | 220M/2.78G [00:01<00:13, 189MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  38%|█▌  | 126M/335M [00:00<00:01, 172MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)ch_model.safetensors:   2%|   | 178M/10.3G [00:00<00:55, 182MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading model.safetensors:  38%|███▊      | 189M/492M [00:01<00:01, 170MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  44%|█▊  | 147M/335M [00:00<00:01, 178MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Downloading model.safetensors:   9%|▊        | 241M/2.78G [00:01<00:13, 187MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  44%|█▊  | 147M/335M [00:00<00:01, 173MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)ch_model.safetensors:   2%|   | 199M/10.3G [00:01<00:55, 181MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading model.safetensors:  43%|████▎     | 210M/492M [00:01<00:01, 169MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  50%|██  | 168M/335M [00:00<00:00, 176MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Downloading model.safetensors:   9%|▊        | 262M/2.78G [00:01<00:13, 182MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  50%|██  | 168M/335M [00:01<00:01, 165MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)ch_model.safetensors:   2%|   | 220M/10.3G [00:01<00:59, 170MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  56%|██▎ | 189M/335M [00:01<00:00, 168MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading model.safetensors:  47%|████▋     | 231M/492M [00:01<00:01, 161MB/s]\u001b[A\u001b[A\u001b[A\n",
      "Downloading model.safetensors:  10%|▉        | 283M/2.78G [00:01<00:14, 177MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  56%|██▎ | 189M/335M [00:01<00:00, 160MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)ch_model.safetensors:   2%|   | 241M/10.3G [00:01<00:57, 173MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  63%|██▌ | 210M/335M [00:01<00:00, 173MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading model.safetensors:  51%|█████     | 252M/492M [00:01<00:01, 155MB/s]\u001b[A\u001b[A\u001b[A\n",
      "Downloading model.safetensors:  11%|▉        | 304M/2.78G [00:01<00:14, 173MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  63%|██▌ | 210M/335M [00:01<00:00, 157MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)ch_model.safetensors:   3%|   | 262M/10.3G [00:01<00:59, 167MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  69%|██▊ | 231M/335M [00:01<00:00, 172MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading model.safetensors:  55%|█████▌    | 273M/492M [00:01<00:01, 155MB/s]\u001b[A\u001b[A\u001b[A\n",
      "Downloading model.safetensors:  12%|█        | 325M/2.78G [00:01<00:14, 171MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)ch_model.safetensors:   3%|   | 283M/10.3G [00:01<00:59, 168MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  69%|██▊ | 231M/335M [00:01<00:00, 157MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  75%|███ | 252M/335M [00:01<00:00, 175MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Downloading model.safetensors:  12%|█        | 346M/2.78G [00:01<00:14, 171MB/s]\u001b[A\n",
      "\n",
      "\n",
      "Downloading model.safetensors:  60%|█████▉    | 294M/492M [00:01<00:01, 156MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)ch_model.safetensors:   3%|   | 304M/10.3G [00:01<00:56, 175MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  81%|███▎| 273M/335M [00:01<00:00, 174MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  75%|███ | 252M/335M [00:01<00:00, 159MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Downloading model.safetensors:  13%|█▏       | 367M/2.78G [00:01<00:14, 171MB/s]\u001b[A\n",
      "\n",
      "\n",
      "Downloading model.safetensors:  64%|██████▍   | 315M/492M [00:01<00:01, 153MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)ch_model.safetensors:   3%|   | 325M/10.3G [00:01<00:58, 171MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  88%|███▌| 294M/335M [00:01<00:00, 171MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  81%|███▎| 273M/335M [00:01<00:00, 158MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Downloading model.safetensors:  14%|█▎       | 388M/2.78G [00:02<00:14, 165MB/s]\u001b[A\n",
      "\n",
      "\n",
      "Downloading model.safetensors:  68%|██████▊   | 336M/492M [00:01<00:01, 152MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)ch_model.safetensors:   3%|   | 346M/10.3G [00:01<01:00, 164MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  94%|███▊| 315M/335M [00:01<00:00, 165MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  88%|███▌| 294M/335M [00:01<00:00, 156MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Downloading model.safetensors:  15%|█▎       | 409M/2.78G [00:02<00:14, 161MB/s]\u001b[A\n",
      "\n",
      "\n",
      "Downloading model.safetensors:  72%|███████▏  | 357M/492M [00:02<00:00, 152MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)ch_model.safetensors:   4%|   | 367M/10.3G [00:02<01:00, 163MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)ch_model.safetensors: 100%|████| 335M/335M [00:01<00:00, 169MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  94%|███▊| 315M/335M [00:01<00:00, 160MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Downloading (…)ch_model.safetensors: 100%|████| 335M/335M [00:02<00:00, 164MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Downloading model.safetensors:  79%|███████▉  | 388M/492M [00:02<00:00, 182MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)ch_model.safetensors:   4%|   | 398M/10.3G [00:02<00:50, 195MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Downloading model.safetensors:  17%|█▌       | 472M/2.78G [00:02<00:10, 223MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)ch_model.safetensors:   4%|▏  | 440M/10.3G [00:02<00:39, 250MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading model.safetensors:  89%|████████▉ | 440M/492M [00:02<00:00, 258MB/s]\u001b[A\u001b[A\u001b[A\n",
      "Downloading model.safetensors:  18%|█▋       | 514M/2.78G [00:02<00:08, 268MB/s]\u001b[A\n",
      "\n",
      "\n",
      "Downloading model.safetensors: 100%|██████████| 492M/492M [00:02<00:00, 199MB/s]\u001b[A\u001b[A\u001b[A\n",
      "Fetching 15 files:  27%|██████▋                  | 4/15 [00:02<00:08,  1.26it/s]\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)ch_model.safetensors:   5%|▏  | 482M/10.3G [00:02<00:35, 273MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Downloading model.safetensors:  20%|█▊       | 556M/2.78G [00:02<00:07, 304MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)ch_model.safetensors:   5%|▏  | 514M/10.3G [00:02<00:37, 260MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Downloading model.safetensors:  22%|█▉       | 598M/2.78G [00:02<00:06, 329MB/s]\u001b[A\n",
      "Downloading model.safetensors:  23%|██       | 640M/2.78G [00:02<00:06, 348MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)ch_model.safetensors:   5%|▏  | 545M/10.3G [00:02<00:38, 253MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Downloading model.safetensors:  25%|██▏      | 682M/2.78G [00:02<00:05, 360MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)ch_model.safetensors:   6%|▏  | 577M/10.3G [00:02<00:39, 246MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Downloading model.safetensors:  26%|██▎      | 724M/2.78G [00:03<00:05, 371MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)ch_model.safetensors:   6%|▏  | 608M/10.3G [00:02<00:39, 242MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Downloading model.safetensors:  28%|██▍      | 765M/2.78G [00:03<00:05, 376MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)ch_model.safetensors:   6%|▏  | 640M/10.3G [00:03<00:37, 256MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Downloading model.safetensors:  29%|██▌      | 807M/2.78G [00:03<00:05, 384MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)ch_model.safetensors:   7%|▏  | 682M/10.3G [00:03<00:32, 295MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Downloading model.safetensors:  31%|██▊      | 849M/2.78G [00:03<00:04, 388MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)ch_model.safetensors:   7%|▏  | 724M/10.3G [00:03<00:29, 327MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Downloading model.safetensors:  32%|██▉      | 891M/2.78G [00:03<00:04, 395MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)ch_model.safetensors:   7%|▏  | 765M/10.3G [00:03<00:27, 350MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Downloading model.safetensors:  34%|███      | 933M/2.78G [00:03<00:04, 396MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)ch_model.safetensors:   8%|▏  | 807M/10.3G [00:03<00:25, 367MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Downloading model.safetensors:  35%|███▏     | 975M/2.78G [00:03<00:04, 394MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)ch_model.safetensors:   8%|▏  | 849M/10.3G [00:03<00:24, 380MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Downloading model.safetensors:  37%|██▉     | 1.02G/2.78G [00:03<00:04, 398MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)ch_model.safetensors:   9%|▎  | 891M/10.3G [00:03<00:23, 391MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Downloading model.safetensors:  38%|███     | 1.06G/2.78G [00:03<00:04, 402MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)ch_model.safetensors:   9%|▎  | 933M/10.3G [00:03<00:23, 396MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Downloading model.safetensors:  40%|███▏    | 1.10G/2.78G [00:03<00:04, 403MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)ch_model.safetensors:   9%|▎  | 975M/10.3G [00:03<00:24, 373MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Downloading model.safetensors:  41%|███▎    | 1.14G/2.78G [00:04<00:04, 335MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  10%|▏ | 1.02G/10.3G [00:04<00:29, 313MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Downloading model.safetensors:  43%|███▍    | 1.18G/2.78G [00:04<00:05, 292MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  10%|▏ | 1.06G/10.3G [00:04<00:32, 283MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Downloading model.safetensors:  44%|███▌    | 1.23G/2.78G [00:04<00:05, 268MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  11%|▏ | 1.09G/10.3G [00:04<00:34, 269MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Downloading model.safetensors:  45%|███▌    | 1.26G/2.78G [00:04<00:05, 254MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  11%|▏ | 1.12G/10.3G [00:04<00:35, 258MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Downloading model.safetensors:  46%|███▋    | 1.29G/2.78G [00:04<00:06, 243MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  11%|▏ | 1.15G/10.3G [00:04<00:36, 251MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  12%|▏ | 1.18G/10.3G [00:04<00:37, 245MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Downloading model.safetensors:  48%|███▊    | 1.32G/2.78G [00:04<00:06, 237MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  12%|▏ | 1.22G/10.3G [00:04<00:37, 240MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Downloading model.safetensors:  49%|███▉    | 1.35G/2.78G [00:05<00:06, 233MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  12%|▏ | 1.25G/10.3G [00:05<00:37, 239MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Downloading model.safetensors:  50%|███▉    | 1.38G/2.78G [00:05<00:06, 231MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  12%|▏ | 1.28G/10.3G [00:05<00:38, 236MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Downloading model.safetensors:  51%|████    | 1.42G/2.78G [00:05<00:06, 227MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  13%|▎ | 1.31G/10.3G [00:05<00:35, 253MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Downloading model.safetensors:  52%|████▏   | 1.45G/2.78G [00:05<00:05, 234MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  13%|▎ | 1.35G/10.3G [00:05<00:30, 292MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Downloading model.safetensors:  54%|████▎   | 1.49G/2.78G [00:05<00:04, 275MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  14%|▎ | 1.39G/10.3G [00:05<00:27, 325MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Downloading model.safetensors:  55%|████▍   | 1.53G/2.78G [00:05<00:04, 308MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  14%|▎ | 1.44G/10.3G [00:05<00:25, 349MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Downloading model.safetensors:  57%|████▌   | 1.57G/2.78G [00:05<00:03, 334MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  14%|▎ | 1.49G/10.3G [00:05<00:24, 360MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Downloading model.safetensors:  58%|████▋   | 1.61G/2.78G [00:05<00:03, 344MB/s]\u001b[A\n",
      "Downloading model.safetensors:  60%|████▊   | 1.66G/2.78G [00:06<00:03, 359MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  15%|▎ | 1.53G/10.3G [00:05<00:28, 310MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Downloading model.safetensors:  61%|████▉   | 1.70G/2.78G [00:06<00:02, 372MB/s]\u001b[A\n",
      "Downloading model.safetensors:  63%|█████   | 1.74G/2.78G [00:06<00:02, 384MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  15%|▎ | 1.57G/10.3G [00:06<00:30, 281MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Downloading model.safetensors:  64%|█████▏  | 1.78G/2.78G [00:06<00:02, 392MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  16%|▎ | 1.60G/10.3G [00:06<00:32, 267MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Downloading model.safetensors:  66%|█████▎  | 1.82G/2.78G [00:06<00:02, 394MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  16%|▎ | 1.64G/10.3G [00:06<00:33, 258MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Downloading model.safetensors:  67%|█████▎  | 1.87G/2.78G [00:06<00:02, 398MB/s]\u001b[A\n",
      "Downloading model.safetensors:  69%|█████▍  | 1.91G/2.78G [00:06<00:02, 401MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  16%|▎ | 1.67G/10.3G [00:06<00:34, 250MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Downloading model.safetensors:  70%|█████▌  | 1.95G/2.78G [00:06<00:02, 401MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  17%|▎ | 1.70G/10.3G [00:06<00:35, 244MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Downloading model.safetensors:  72%|█████▋  | 1.99G/2.78G [00:06<00:01, 403MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  17%|▎ | 1.73G/10.3G [00:06<00:35, 241MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Downloading model.safetensors:  73%|█████▊  | 2.03G/2.78G [00:06<00:01, 405MB/s]\u001b[A\n",
      "Downloading model.safetensors:  75%|█████▉  | 2.08G/2.78G [00:07<00:01, 408MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  17%|▎ | 1.76G/10.3G [00:06<00:35, 239MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Downloading model.safetensors:  76%|██████  | 2.12G/2.78G [00:07<00:01, 408MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  17%|▎ | 1.79G/10.3G [00:07<00:35, 236MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Downloading model.safetensors:  78%|██████▏ | 2.16G/2.78G [00:07<00:01, 411MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  18%|▎ | 1.82G/10.3G [00:07<00:36, 234MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Downloading model.safetensors:  79%|██████▎ | 2.20G/2.78G [00:07<00:01, 408MB/s]\u001b[A\n",
      "Downloading model.safetensors:  81%|██████▍ | 2.24G/2.78G [00:07<00:01, 407MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  18%|▎ | 1.86G/10.3G [00:07<00:36, 233MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Downloading model.safetensors:  82%|██████▌ | 2.29G/2.78G [00:07<00:01, 402MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  18%|▎ | 1.89G/10.3G [00:07<00:35, 234MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Downloading model.safetensors:  84%|██████▋ | 2.33G/2.78G [00:07<00:01, 398MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  19%|▎ | 1.92G/10.3G [00:07<00:36, 232MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Downloading model.safetensors:  85%|██████▊ | 2.37G/2.78G [00:07<00:01, 399MB/s]\u001b[A\n",
      "Downloading model.safetensors:  87%|██████▉ | 2.41G/2.78G [00:07<00:00, 403MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  19%|▍ | 1.95G/10.3G [00:07<00:35, 233MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Downloading model.safetensors:  88%|███████ | 2.45G/2.78G [00:08<00:00, 401MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  19%|▍ | 1.98G/10.3G [00:07<00:35, 233MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Downloading model.safetensors:  90%|███████▏| 2.50G/2.78G [00:08<00:00, 400MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  20%|▍ | 2.01G/10.3G [00:08<00:35, 233MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Downloading model.safetensors:  92%|███████▎| 2.55G/2.78G [00:08<00:00, 404MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  20%|▍ | 2.04G/10.3G [00:08<00:35, 233MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Downloading model.safetensors:  93%|███████▍| 2.59G/2.78G [00:08<00:00, 400MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  20%|▍ | 2.08G/10.3G [00:08<00:35, 233MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Downloading model.safetensors:  95%|███████▌| 2.63G/2.78G [00:08<00:00, 400MB/s]\u001b[A\n",
      "Downloading model.safetensors:  96%|███████▋| 2.67G/2.78G [00:08<00:00, 405MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  21%|▍ | 2.11G/10.3G [00:08<00:35, 232MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Downloading model.safetensors:  98%|███████▊| 2.72G/2.78G [00:08<00:00, 406MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  21%|▍ | 2.14G/10.3G [00:08<00:34, 232MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Downloading model.safetensors: 100%|████████| 2.78G/2.78G [00:08<00:00, 315MB/s]\u001b[A\n",
      "Fetching 15 files:  40%|██████████               | 6/15 [00:09<00:16,  1.86s/it]\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  21%|▍ | 2.17G/10.3G [00:08<00:34, 232MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  21%|▍ | 2.20G/10.3G [00:08<00:34, 232MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  22%|▍ | 2.23G/10.3G [00:08<00:34, 232MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  22%|▍ | 2.26G/10.3G [00:09<00:34, 231MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  22%|▍ | 2.30G/10.3G [00:09<00:34, 231MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  23%|▍ | 2.33G/10.3G [00:09<00:34, 231MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  23%|▍ | 2.36G/10.3G [00:09<00:34, 231MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  23%|▍ | 2.39G/10.3G [00:09<00:34, 232MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  24%|▍ | 2.42G/10.3G [00:09<00:33, 232MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  24%|▍ | 2.45G/10.3G [00:09<00:33, 230MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  24%|▍ | 2.49G/10.3G [00:10<00:33, 230MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  25%|▍ | 2.52G/10.3G [00:10<00:33, 230MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  25%|▍ | 2.55G/10.3G [00:10<00:33, 231MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  25%|▌ | 2.58G/10.3G [00:10<00:33, 231MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  25%|▌ | 2.61G/10.3G [00:10<00:33, 231MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  26%|▌ | 2.64G/10.3G [00:10<00:32, 231MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  26%|▌ | 2.67G/10.3G [00:10<00:32, 231MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  26%|▌ | 2.71G/10.3G [00:11<00:32, 231MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  27%|▌ | 2.74G/10.3G [00:11<00:32, 231MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  27%|▌ | 2.77G/10.3G [00:11<00:32, 231MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  27%|▌ | 2.80G/10.3G [00:11<00:32, 232MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  28%|▌ | 2.83G/10.3G [00:11<00:32, 232MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  28%|▌ | 2.86G/10.3G [00:11<00:31, 232MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  28%|▌ | 2.89G/10.3G [00:11<00:31, 232MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  28%|▌ | 2.93G/10.3G [00:11<00:31, 231MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  29%|▌ | 2.96G/10.3G [00:12<00:31, 232MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Fetching 15 files:  73%|█████████████████▌      | 11/15 [00:22<00:08,  2.06s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "model is not found as a file or in Hugging Face, perhaps file name is wrong? / 指定したモデル名のファイル、またはHugging Faceのモデルが見つかりません。ファイル名が誤っているかもしれません: stabilityai/stable-diffusion-xl-base-1.0\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/urllib3/response.py\", line 444, in _error_catcher\n",
      "    yield\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/urllib3/response.py\", line 567, in read\n",
      "    data = self._fp_read(amt) if not fp_closed else b\"\"\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/urllib3/response.py\", line 533, in _fp_read\n",
      "    return self._fp.read(amt) if amt is not None else self._fp.read()\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/http/client.py\", line 466, in read\n",
      "    s = self.fp.read(amt)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/socket.py\", line 705, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/ssl.py\", line 1274, in recv_into\n",
      "    return self.read(nbytes, buffer)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/ssl.py\", line 1130, in read\n",
      "    return self._sslobj.read(len, buffer)\n",
      "TimeoutError: The read operation timed out\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/requests/models.py\", line 816, in generate\n",
      "    yield from self.raw.stream(chunk_size, decode_content=True)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/urllib3/response.py\", line 628, in stream\n",
      "    data = self.read(amt=amt, decode_content=decode_content)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/urllib3/response.py\", line 566, in read\n",
      "    with self._error_catcher():\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/contextlib.py\", line 153, in __exit__\n",
      "    self.gen.throw(typ, value, traceback)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/urllib3/response.py\", line 449, in _error_catcher\n",
      "    raise ReadTimeoutError(self._pool, None, \"Read timed out.\")\n",
      "urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='cdn-lfs.huggingface.co', port=443): Read timed out.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ec2-user/SageMaker/sd-scripts/sdxl_train_network.py\", line 176, in <module>\n",
      "    trainer.train(args)\n",
      "  File \"/home/ec2-user/SageMaker/sd-scripts/train_network.py\", line 214, in train\n",
      "    model_version, text_encoder, vae, unet = self.load_target_model(args, weight_dtype, accelerator)\n",
      "  File \"/home/ec2-user/SageMaker/sd-scripts/sdxl_train_network.py\", line 37, in load_target_model\n",
      "    ) = sdxl_train_util.load_target_model(args, accelerator, sdxl_model_util.MODEL_VERSION_SDXL_BASE_V1_0, weight_dtype)\n",
      "  File \"/home/ec2-user/SageMaker/sd-scripts/library/sdxl_train_util.py\", line 33, in load_target_model\n",
      "    ) = _load_target_model(\n",
      "  File \"/home/ec2-user/SageMaker/sd-scripts/library/sdxl_train_util.py\", line 92, in _load_target_model\n",
      "    raise ex\n",
      "  File \"/home/ec2-user/SageMaker/sd-scripts/library/sdxl_train_util.py\", line 87, in _load_target_model\n",
      "    raise ex\n",
      "  File \"/home/ec2-user/SageMaker/sd-scripts/library/sdxl_train_util.py\", line 79, in _load_target_model\n",
      "    pipe = StableDiffusionXLPipeline.from_pretrained(\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/diffusers/pipelines/pipeline_utils.py\", line 908, in from_pretrained\n",
      "    cached_folder = cls.download(\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/diffusers/pipelines/pipeline_utils.py\", line 1488, in download\n",
      "    cached_folder = snapshot_download(\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py\", line 118, in _inner_fn\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/huggingface_hub/_snapshot_download.py\", line 235, in snapshot_download\n",
      "    thread_map(\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/tqdm/contrib/concurrent.py\", line 69, in thread_map\n",
      "    return _executor_map(ThreadPoolExecutor, fn, *iterables, **tqdm_kwargs)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/tqdm/contrib/concurrent.py\", line 51, in _executor_map\n",
      "    return list(tqdm_class(ex.map(fn, *iterables, chunksize=chunksize), **kwargs))\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/tqdm/std.py\", line 1178, in __iter__\n",
      "    for obj in iterable:\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/concurrent/futures/_base.py\", line 621, in result_iterator\n",
      "    yield _result_or_cancel(fs.pop())\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/concurrent/futures/_base.py\", line 319, in _result_or_cancel\n",
      "    return fut.result(timeout)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/concurrent/futures/_base.py\", line 458, in result\n",
      "    return self.__get_result()\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
      "    raise self._exception\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\n",
      "    result = self.fn(*self.args, **self.kwargs)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/huggingface_hub/_snapshot_download.py\", line 211, in _inner_hf_hub_download\n",
      "    return hf_hub_download(\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py\", line 118, in _inner_fn\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1364, in hf_hub_download\n",
      "    http_get(\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 541, in http_get\n",
      "    for chunk in r.iter_content(chunk_size=10 * 1024 * 1024):\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/requests/models.py\", line 822, in generate\n",
      "    raise ConnectionError(e)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='cdn-lfs.huggingface.co', port=443): Read timed out.\n",
      "Downloading (…)ch_model.safetensors:  29%|▌ | 2.99G/10.3G [00:22<00:55, 131MB/s]\n",
      "WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 21900 closing signal SIGTERM\n",
      "WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 21901 closing signal SIGTERM\n",
      "WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 21902 closing signal SIGTERM\n",
      "ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 21899) of binary: /home/ec2-user/anaconda3/envs/pytorch_p310/bin/python3.10\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/bin/accelerate\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/accelerate/commands/accelerate_cli.py\", line 45, in main\n",
      "    args.func(args)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/accelerate/commands/launch.py\", line 909, in launch_command\n",
      "    multi_gpu_launcher(args)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/accelerate/commands/launch.py\", line 604, in multi_gpu_launcher\n",
      "    distrib_run.run(args)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/distributed/run.py\", line 785, in run\n",
      "    elastic_launch(\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 134, in __call__\n",
      "    return launch_agent(self._config, self._entrypoint, list(args))\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 250, in launch_agent\n",
      "    raise ChildFailedError(\n",
      "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
      "============================================================\n",
      "sdxl_train_network.py FAILED\n",
      "------------------------------------------------------------\n",
      "Failures:\n",
      "  <NO_OTHER_FAILURES>\n",
      "------------------------------------------------------------\n",
      "Root Cause (first observed failure):\n",
      "[0]:\n",
      "  time      : 2023-08-31_09:09:39\n",
      "  host      : ip-172-16-61-251.us-west-2.compute.internal\n",
      "  rank      : 0 (local_rank: 0)\n",
      "  exitcode  : 1 (pid: 21899)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "!accelerate launch --multi_gpu sdxl_train_network.py \\\n",
    "   --pretrained_model_name_or_path=\"stabilityai/stable-diffusion-xl-base-1.0\" \\\n",
    "   --dataset_config=$DATASET_CONFIG \\\n",
    "   --output_dir=$LORA_WEIGHT \\\n",
    "   --network_module=\"networks.lora\" \\\n",
    "   --max_train_epochs=10 \\\n",
    "   --network_train_unet_only \\\n",
    "   --learning_rate=4e-4 \\\n",
    "   --lr_scheduler=\"constant\" \\\n",
    "   --lr_scheduler_num_cycles=1 \\\n",
    "   #--lr_scheduler_num_cycles=10 \\\n",
    "   --network_dim=128 \\\n",
    "   #--network_alpha=1 \\\n",
    "   --network_alpha=32 \\\n",
    "   --output_name=\"lora-aa-256\" \\\n",
    "   --save_every_n_epochs=10 \\\n",
    "   --mixed_precision=\"fp16\" \\\n",
    "   #--mixed_precision=\"bf16\" \\\n",
    "   #--save_precision=\"bf16\" \\\n",
    "   --gradient_checkpointing \\\n",
    "   --prior_loss_weight=1 \\\n",
    "   --max_token_length=225 \\\n",
    "   --save_model_as=\"safetensors\" \\\n",
    "   --no_half_vae \\\n",
    "   --xformers \\\n",
    "   --optimizer_type=\"prodigy\" \\\n",
    "   #--optimizer_type=\"Adafactor\" \\\n",
    "   #--sample_every_n_epochs=10 \\\n",
    "   #--sample_prompts=\"./sample_prompts.txt\" \\\n",
    "   #--sample_sampler=\"euler_a\" \\\n",
    "   --logging_dir=\"./logs\" \\\n",
    "   --log_with=\"all\" \\\n",
    "   --wandb_api_key=\"245e51c77454f92bbb80ff882df76d02cfd7f4a5\" \\\n",
    "   # --text_encoder_lr=4e-4 \\\n",
    "   # --unet_lr=4e-4 \\\n",
    "   # --cache_latents_to_disk \\\n",
    "   # --optimizer_args \\\n",
    "   # --max_data_loader_n_worker=0 \\\n",
    "   # --noise_offset=0.0 \\\n",
    "   --log_tracker_name=\"lora-aa-256\" # register a user via https://wandb.ai and get an API key \n",
    "   # --cache_text_encoder_outputs_to_disk \\\n",
    "   # --save_model_as=\"safetensors\" \\"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb306015-992c-4eaf-b8e0-c8386b875344",
   "metadata": {},
   "source": [
    "# Below are testing, not to run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d5a900-c89b-45ff-b24a-cad912c54865",
   "metadata": {},
   "source": [
    "## LoRA Training with single gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7449ff23-d4bf-4bf4-9f80-6d68a972bcf4",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prepare tokenizers\n",
      "prepare tokenizers\n",
      "prepare tokenizers\n",
      "prepare tokenizers\n",
      "update token length: 225\n",
      "Loading dataset config from aa/dataset.toml\n",
      "update token length: 225\n",
      "Loading dataset config from aa/dataset.toml\n",
      "update token length: 225\n",
      "Loading dataset config from aa/dataset.toml\n",
      "update token length: 225\n",
      "Loading dataset config from aa/dataset.toml\n",
      "prepare images.\n",
      "found directory ./aa/images/sample contains 10 image files\n",
      "neither caption file nor class tokens are found. use empty caption for ./aa/images/sample/IMG_1421.jpg / キャプションファイルもclass tokenも見つかりませんでした。空のキャプションを使用します: ./aa/images/sample/IMG_1421.jpg\n",
      "neither caption file nor class tokens are found. use empty caption for ./aa/images/sample/IMG_1556.jpg / キャプションファイルもclass tokenも見つかりませんでした。空のキャプションを使用します: ./aa/images/sample/IMG_1556.jpg\n",
      "neither caption file nor class tokens are found. use empty caption for ./aa/images/sample/IMG_1579.jpg / キャプションファイルもclass tokenも見つかりませんでした。空のキャプションを使用します: ./aa/images/sample/IMG_1579.jpg\n",
      "neither caption file nor class tokens are found. use empty caption for ./aa/images/sample/IMG_1752.jpg / キャプションファイルもclass tokenも見つかりませんでした。空のキャプションを使用します: ./aa/images/sample/IMG_1752.jpg\n",
      "neither caption file nor class tokens are found. use empty caption for ./aa/images/sample/IMG_1763.jpg / キャプションファイルもclass tokenも見つかりませんでした。空のキャプションを使用します: ./aa/images/sample/IMG_1763.jpg\n",
      "neither caption file nor class tokens are found. use empty caption for ./aa/images/sample/IMG_1764.jpg / キャプションファイルもclass tokenも見つかりませんでした。空のキャプションを使用します: ./aa/images/sample/IMG_1764.jpg\n",
      "neither caption file nor class tokens are found. use empty caption for ./aa/images/sample/IMG_5258.jpg / キャプションファイルもclass tokenも見つかりませんでした。空のキャプションを使用します: ./aa/images/sample/IMG_5258.jpg\n",
      "neither caption file nor class tokens are found. use empty caption for ./aa/images/sample/IMG_7511.jpg / キャプションファイルもclass tokenも見つかりませんでした。空のキャプションを使用します: ./aa/images/sample/IMG_7511.jpg\n",
      "neither caption file nor class tokens are found. use empty caption for ./aa/images/sample/IMG_9133.jpg / キャプションファイルもclass tokenも見つかりませんでした。空のキャプションを使用します: ./aa/images/sample/IMG_9133.jpg\n",
      "neither caption file nor class tokens are found. use empty caption for ./aa/images/sample/IMG_9674.jpg / キャプションファイルもclass tokenも見つかりませんでした。空のキャプションを使用します: ./aa/images/sample/IMG_9674.jpg\n",
      "No caption file found for 10 images. Training will continue without captions for these images. If class token exists, it will be used. / 10枚の画像にキャプションファイルが見つかりませんでした。これらの画像についてはキャプションなしで学習を続行します。class tokenが存在する場合はそれを使います。\n",
      "./aa/images/sample/IMG_1421.jpg\n",
      "./aa/images/sample/IMG_1556.jpg\n",
      "./aa/images/sample/IMG_1579.jpg\n",
      "./aa/images/sample/IMG_1752.jpg\n",
      "./aa/images/sample/IMG_1763.jpg\n",
      "./aa/images/sample/IMG_1764.jpg... and 5 more\n",
      "200 train images with repeating.\n",
      "0 reg images.\n",
      "no regularization images / 正則化画像が見つかりませんでした\n",
      "[Dataset 0]\n",
      "  batch_size: 1\n",
      "  resolution: (1024, 1024)\n",
      "  enable_bucket: True\n",
      "  min_bucket_reso: 256\n",
      "  max_bucket_reso: 1024\n",
      "  bucket_reso_steps: 64\n",
      "  bucket_no_upscale: False\n",
      "\n",
      "  [Subset 0 of Dataset 0]\n",
      "    image_dir: \"./aa/images/sample\"\n",
      "    image_count: 10\n",
      "    num_repeats: 20\n",
      "    shuffle_caption: True\n",
      "    keep_tokens: 0\n",
      "    caption_dropout_rate: 0.0\n",
      "    caption_dropout_every_n_epoches: 0\n",
      "    caption_tag_dropout_rate: 0.0\n",
      "    color_aug: False\n",
      "    flip_aug: False\n",
      "    face_crop_aug_range: None\n",
      "    random_crop: False\n",
      "    token_warmup_min: 1,\n",
      "    token_warmup_step: 0,\n",
      "    is_reg: False\n",
      "    class_tokens: None\n",
      "    caption_extension: .txt\n",
      "\n",
      "\n",
      "[Dataset 0]\n",
      "loading image sizes.\n",
      "100%|█████████████████████████████████████████| 10/10 [00:00<00:00, 3121.46it/s]\n",
      "make buckets\n",
      "number of images (including repeats) / 各bucketの画像枚数（繰り返し回数を含む）\n",
      "bucket 0: resolution (768, 1024), count: 100\n",
      "bucket 1: resolution (832, 1024), count: 20\n",
      "bucket 2: resolution (896, 1024), count: 20\n",
      "bucket 3: resolution (1024, 1024), count: 60\n",
      "mean ar error (without repeats): 0.012643312395322126\n",
      "preparing accelerator\n",
      "prepare images.\n",
      "found directory ./aa/images/sample contains 10 image files\n",
      "neither caption file nor class tokens are found. use empty caption for ./aa/images/sample/IMG_1421.jpg / キャプションファイルもclass tokenも見つかりませんでした。空のキャプションを使用します: ./aa/images/sample/IMG_1421.jpg\n",
      "neither caption file nor class tokens are found. use empty caption for ./aa/images/sample/IMG_1556.jpg / キャプションファイルもclass tokenも見つかりませんでした。空のキャプションを使用します: ./aa/images/sample/IMG_1556.jpg\n",
      "neither caption file nor class tokens are found. use empty caption for ./aa/images/sample/IMG_1579.jpg / キャプションファイルもclass tokenも見つかりませんでした。空のキャプションを使用します: ./aa/images/sample/IMG_1579.jpg\n",
      "neither caption file nor class tokens are found. use empty caption for ./aa/images/sample/IMG_1752.jpg / キャプションファイルもclass tokenも見つかりませんでした。空のキャプションを使用します: ./aa/images/sample/IMG_1752.jpg\n",
      "neither caption file nor class tokens are found. use empty caption for ./aa/images/sample/IMG_1763.jpg / キャプションファイルもclass tokenも見つかりませんでした。空のキャプションを使用します: ./aa/images/sample/IMG_1763.jpg\n",
      "neither caption file nor class tokens are found. use empty caption for ./aa/images/sample/IMG_1764.jpg / キャプションファイルもclass tokenも見つかりませんでした。空のキャプションを使用します: ./aa/images/sample/IMG_1764.jpg\n",
      "neither caption file nor class tokens are found. use empty caption for ./aa/images/sample/IMG_5258.jpg / キャプションファイルもclass tokenも見つかりませんでした。空のキャプションを使用します: ./aa/images/sample/IMG_5258.jpg\n",
      "neither caption file nor class tokens are found. use empty caption for ./aa/images/sample/IMG_7511.jpg / キャプションファイルもclass tokenも見つかりませんでした。空のキャプションを使用します: ./aa/images/sample/IMG_7511.jpg\n",
      "neither caption file nor class tokens are found. use empty caption for ./aa/images/sample/IMG_9133.jpg / キャプションファイルもclass tokenも見つかりませんでした。空のキャプションを使用します: ./aa/images/sample/IMG_9133.jpg\n",
      "neither caption file nor class tokens are found. use empty caption for ./aa/images/sample/IMG_9674.jpg / キャプションファイルもclass tokenも見つかりませんでした。空のキャプションを使用します: ./aa/images/sample/IMG_9674.jpg\n",
      "No caption file found for 10 images. Training will continue without captions for these images. If class token exists, it will be used. / 10枚の画像にキャプションファイルが見つかりませんでした。これらの画像についてはキャプションなしで学習を続行します。class tokenが存在する場合はそれを使います。\n",
      "./aa/images/sample/IMG_1421.jpg\n",
      "./aa/images/sample/IMG_1556.jpg\n",
      "./aa/images/sample/IMG_1579.jpg\n",
      "./aa/images/sample/IMG_1752.jpg\n",
      "./aa/images/sample/IMG_1763.jpg\n",
      "./aa/images/sample/IMG_1764.jpg... and 5 more\n",
      "200 train images with repeating.\n",
      "0 reg images.\n",
      "no regularization images / 正則化画像が見つかりませんでした\n",
      "[Dataset 0]\n",
      "  batch_size: 1\n",
      "  resolution: (1024, 1024)\n",
      "  enable_bucket: True\n",
      "  min_bucket_reso: 256\n",
      "  max_bucket_reso: 1024\n",
      "  bucket_reso_steps: 64\n",
      "  bucket_no_upscale: False\n",
      "\n",
      "  [Subset 0 of Dataset 0]\n",
      "    image_dir: \"./aa/images/sample\"\n",
      "    image_count: 10\n",
      "    num_repeats: 20\n",
      "    shuffle_caption: True\n",
      "    keep_tokens: 0\n",
      "    caption_dropout_rate: 0.0\n",
      "    caption_dropout_every_n_epoches: 0\n",
      "    caption_tag_dropout_rate: 0.0\n",
      "    color_aug: False\n",
      "    flip_aug: False\n",
      "    face_crop_aug_range: None\n",
      "    random_crop: False\n",
      "    token_warmup_min: 1,\n",
      "    token_warmup_step: 0,\n",
      "    is_reg: False\n",
      "    class_tokens: None\n",
      "    caption_extension: .txt\n",
      "\n",
      "\n",
      "[Dataset 0]\n",
      "loading image sizes.\n",
      "100%|█████████████████████████████████████████| 10/10 [00:00<00:00, 3165.99it/s]\n",
      "make buckets\n",
      "number of images (including repeats) / 各bucketの画像枚数（繰り返し回数を含む）\n",
      "bucket 0: resolution (768, 1024), count: 100\n",
      "bucket 1: resolution (832, 1024), count: 20\n",
      "bucket 2: resolution (896, 1024), count: 20\n",
      "bucket 3: resolution (1024, 1024), count: 60\n",
      "mean ar error (without repeats): 0.012643312395322126\n",
      "prepare images.\n",
      "preparing accelerator\n",
      "found directory ./aa/images/sample contains 10 image files\n",
      "neither caption file nor class tokens are found. use empty caption for ./aa/images/sample/IMG_1421.jpg / キャプションファイルもclass tokenも見つかりませんでした。空のキャプションを使用します: ./aa/images/sample/IMG_1421.jpg\n",
      "neither caption file nor class tokens are found. use empty caption for ./aa/images/sample/IMG_1556.jpg / キャプションファイルもclass tokenも見つかりませんでした。空のキャプションを使用します: ./aa/images/sample/IMG_1556.jpg\n",
      "neither caption file nor class tokens are found. use empty caption for ./aa/images/sample/IMG_1579.jpg / キャプションファイルもclass tokenも見つかりませんでした。空のキャプションを使用します: ./aa/images/sample/IMG_1579.jpg\n",
      "neither caption file nor class tokens are found. use empty caption for ./aa/images/sample/IMG_1752.jpg / キャプションファイルもclass tokenも見つかりませんでした。空のキャプションを使用します: ./aa/images/sample/IMG_1752.jpg\n",
      "neither caption file nor class tokens are found. use empty caption for ./aa/images/sample/IMG_1763.jpg / キャプションファイルもclass tokenも見つかりませんでした。空のキャプションを使用します: ./aa/images/sample/IMG_1763.jpg\n",
      "neither caption file nor class tokens are found. use empty caption for ./aa/images/sample/IMG_1764.jpg / キャプションファイルもclass tokenも見つかりませんでした。空のキャプションを使用します: ./aa/images/sample/IMG_1764.jpg\n",
      "neither caption file nor class tokens are found. use empty caption for ./aa/images/sample/IMG_5258.jpg / キャプションファイルもclass tokenも見つかりませんでした。空のキャプションを使用します: ./aa/images/sample/IMG_5258.jpg\n",
      "neither caption file nor class tokens are found. use empty caption for ./aa/images/sample/IMG_7511.jpg / キャプションファイルもclass tokenも見つかりませんでした。空のキャプションを使用します: ./aa/images/sample/IMG_7511.jpg\n",
      "neither caption file nor class tokens are found. use empty caption for ./aa/images/sample/IMG_9133.jpg / キャプションファイルもclass tokenも見つかりませんでした。空のキャプションを使用します: ./aa/images/sample/IMG_9133.jpg\n",
      "neither caption file nor class tokens are found. use empty caption for ./aa/images/sample/IMG_9674.jpg / キャプションファイルもclass tokenも見つかりませんでした。空のキャプションを使用します: ./aa/images/sample/IMG_9674.jpg\n",
      "No caption file found for 10 images. Training will continue without captions for these images. If class token exists, it will be used. / 10枚の画像にキャプションファイルが見つかりませんでした。これらの画像についてはキャプションなしで学習を続行します。class tokenが存在する場合はそれを使います。\n",
      "./aa/images/sample/IMG_1421.jpg\n",
      "./aa/images/sample/IMG_1556.jpg\n",
      "./aa/images/sample/IMG_1579.jpg\n",
      "./aa/images/sample/IMG_1752.jpg\n",
      "./aa/images/sample/IMG_1763.jpg\n",
      "./aa/images/sample/IMG_1764.jpg... and 5 more\n",
      "200 train images with repeating.\n",
      "0 reg images.\n",
      "no regularization images / 正則化画像が見つかりませんでした\n",
      "[Dataset 0]\n",
      "  batch_size: 1\n",
      "  resolution: (1024, 1024)\n",
      "  enable_bucket: True\n",
      "  min_bucket_reso: 256\n",
      "  max_bucket_reso: 1024\n",
      "  bucket_reso_steps: 64\n",
      "  bucket_no_upscale: False\n",
      "\n",
      "  [Subset 0 of Dataset 0]\n",
      "    image_dir: \"./aa/images/sample\"\n",
      "    image_count: 10\n",
      "    num_repeats: 20\n",
      "    shuffle_caption: True\n",
      "    keep_tokens: 0\n",
      "    caption_dropout_rate: 0.0\n",
      "    caption_dropout_every_n_epoches: 0\n",
      "    caption_tag_dropout_rate: 0.0\n",
      "    color_aug: False\n",
      "    flip_aug: False\n",
      "    face_crop_aug_range: None\n",
      "    random_crop: False\n",
      "    token_warmup_min: 1,\n",
      "    token_warmup_step: 0,\n",
      "    is_reg: False\n",
      "    class_tokens: None\n",
      "    caption_extension: .txt\n",
      "\n",
      "\n",
      "[Dataset 0]\n",
      "loading image sizes.\n",
      "100%|█████████████████████████████████████████| 10/10 [00:00<00:00, 3204.45it/s]\n",
      "make buckets\n",
      "number of images (including repeats) / 各bucketの画像枚数（繰り返し回数を含む）\n",
      "bucket 0: resolution (768, 1024), count: 100\n",
      "bucket 1: resolution (832, 1024), count: 20\n",
      "bucket 2: resolution (896, 1024), count: 20\n",
      "bucket 3: resolution (1024, 1024), count: 60\n",
      "mean ar error (without repeats): 0.012643312395322126\n",
      "preparing accelerator\n",
      "prepare images.\n",
      "found directory ./aa/images/sample contains 10 image files\n",
      "neither caption file nor class tokens are found. use empty caption for ./aa/images/sample/IMG_1421.jpg / キャプションファイルもclass tokenも見つかりませんでした。空のキャプションを使用します: ./aa/images/sample/IMG_1421.jpg\n",
      "neither caption file nor class tokens are found. use empty caption for ./aa/images/sample/IMG_1556.jpg / キャプションファイルもclass tokenも見つかりませんでした。空のキャプションを使用します: ./aa/images/sample/IMG_1556.jpg\n",
      "neither caption file nor class tokens are found. use empty caption for ./aa/images/sample/IMG_1579.jpg / キャプションファイルもclass tokenも見つかりませんでした。空のキャプションを使用します: ./aa/images/sample/IMG_1579.jpg\n",
      "neither caption file nor class tokens are found. use empty caption for ./aa/images/sample/IMG_1752.jpg / キャプションファイルもclass tokenも見つかりませんでした。空のキャプションを使用します: ./aa/images/sample/IMG_1752.jpg\n",
      "neither caption file nor class tokens are found. use empty caption for ./aa/images/sample/IMG_1763.jpg / キャプションファイルもclass tokenも見つかりませんでした。空のキャプションを使用します: ./aa/images/sample/IMG_1763.jpg\n",
      "neither caption file nor class tokens are found. use empty caption for ./aa/images/sample/IMG_1764.jpg / キャプションファイルもclass tokenも見つかりませんでした。空のキャプションを使用します: ./aa/images/sample/IMG_1764.jpg\n",
      "neither caption file nor class tokens are found. use empty caption for ./aa/images/sample/IMG_5258.jpg / キャプションファイルもclass tokenも見つかりませんでした。空のキャプションを使用します: ./aa/images/sample/IMG_5258.jpg\n",
      "neither caption file nor class tokens are found. use empty caption for ./aa/images/sample/IMG_7511.jpg / キャプションファイルもclass tokenも見つかり��せんでした。空のキャプションを使用します: ./aa/images/sample/IMG_7511.jpg\n",
      "neither caption file nor class tokens are found. use empty caption for ./aa/images/sample/IMG_9133.jpg / キャプションファイルもclass tokenも見つかりませんでした。空のキャプションを使用します: ./aa/images/sample/IMG_9133.jpg\n",
      "neither caption file nor class tokens are found. use empty caption for ./aa/images/sample/IMG_9674.jpg / キャプションファイルもclass tokenも見つかりませんでした。空のキャプションを使用します: ./aa/images/sample/IMG_9674.jpg\n",
      "No caption file found for 10 images. Training will continue without captions for these images. If class token exists, it will be used. / 10枚の画像にキャプションファイルが見つかりませんでした。これらの画像についてはキャプションなしで学習を続行します。class tokenが存在する場合はそれを使います。\n",
      "./aa/images/sample/IMG_1421.jpg\n",
      "./aa/images/sample/IMG_1556.jpg\n",
      "./aa/images/sample/IMG_1579.jpg\n",
      "./aa/images/sample/IMG_1752.jpg\n",
      "./aa/images/sample/IMG_1763.jpg\n",
      "./aa/images/sample/IMG_1764.jpg... and 5 more\n",
      "200 train images with repeating.\n",
      "0 reg images.\n",
      "no regularization images / 正則化画像が見つかりませんでした\n",
      "[Dataset 0]\n",
      "  batch_size: 1\n",
      "  resolution: (1024, 1024)\n",
      "  enable_bucket: True\n",
      "  min_bucket_reso: 256\n",
      "  max_bucket_reso: 1024\n",
      "  bucket_reso_steps: 64\n",
      "  bucket_no_upscale: False\n",
      "\n",
      "  [Subset 0 of Dataset 0]\n",
      "    image_dir: \"./aa/images/sample\"\n",
      "    image_count: 10\n",
      "    num_repeats: 20\n",
      "    shuffle_caption: True\n",
      "    keep_tokens: 0\n",
      "    caption_dropout_rate: 0.0\n",
      "    caption_dropout_every_n_epoches: 0\n",
      "    caption_tag_dropout_rate: 0.0\n",
      "    color_aug: False\n",
      "    flip_aug: False\n",
      "    face_crop_aug_range: None\n",
      "    random_crop: False\n",
      "    token_warmup_min: 1,\n",
      "    token_warmup_step: 0,\n",
      "    is_reg: False\n",
      "    class_tokens: None\n",
      "    caption_extension: .txt\n",
      "\n",
      "\n",
      "[Dataset 0]\n",
      "loading image sizes.\n",
      "100%|█████████████████████████████████████████| 10/10 [00:00<00:00, 3206.41it/s]\n",
      "make buckets\n",
      "number of images (including repeats) / 各bucketの画像枚数（繰り返し回数を含む）\n",
      "bucket 0: resolution (768, 1024), count: 100\n",
      "bucket 1: resolution (832, 1024), count: 20\n",
      "bucket 2: resolution (896, 1024), count: 20\n",
      "bucket 3: resolution (1024, 1024), count: 60\n",
      "mean ar error (without repeats): 0.012643312395322126\n",
      "preparing accelerator\n",
      "loading model for process 0/4\n",
      "load Diffusers pretrained models: stabilityai/stable-diffusion-xl-base-1.0, variant=fp16\n",
      "Loading pipeline components...: 100%|█████████████| 6/6 [00:01<00:00,  3.81it/s]\n",
      "U-Net converted to original U-Net\n",
      "loading model for process 1/4\n",
      "load Diffusers pretrained models: stabilityai/stable-diffusion-xl-base-1.0, variant=fp16\n",
      "Loading pipeline components...: 100%|█████████████| 6/6 [00:01<00:00,  3.87it/s]\n",
      "U-Net converted to original U-Net\n",
      "loading model for process 2/4\n",
      "load Diffusers pretrained models: stabilityai/stable-diffusion-xl-base-1.0, variant=fp16\n",
      "Loading pipeline components...: 100%|█████████████| 6/6 [00:01<00:00,  3.82it/s]\n",
      "U-Net converted to original U-Net\n",
      "loading model for process 3/4\n",
      "load Diffusers pretrained models: stabilityai/stable-diffusion-xl-base-1.0, variant=fp16\n",
      "Loading pipeline components...: 100%|█████████████| 6/6 [00:01<00:00,  3.81it/s]\n",
      "U-Net converted to original U-Net\n",
      "Enable xformers for U-NetEnable xformers for U-Net\n",
      "\n",
      "Enable xformers for U-Net\n",
      "Enable xformers for U-Net\n",
      "import network module: networks.lora\n",
      "create LoRA network. base dim (rank): 128, alpha: 32.0\n",
      "neuron dropout: p=None, rank dropout: p=None, module dropout: p=None\n",
      "create LoRA for Text Encoder 1:\n",
      "create LoRA network. base dim (rank): 128, alpha: 32.0\n",
      "neuron dropout: p=None, rank dropout: p=None, module dropout: p=None\n",
      "create LoRA for Text Encoder 1:\n",
      "create LoRA network. base dim (rank): 128, alpha: 32.0\n",
      "neuron dropout: p=None, rank dropout: p=None, module dropout: p=None\n",
      "create LoRA for Text Encoder 1:\n",
      "create LoRA network. base dim (rank): 128, alpha: 32.0\n",
      "neuron dropout: p=None, rank dropout: p=None, module dropout: p=None\n",
      "create LoRA for Text Encoder 1:\n",
      "create LoRA for Text Encoder 2:\n",
      "create LoRA for Text Encoder 2:\n",
      "create LoRA for Text Encoder 2:\n",
      "create LoRA for Text Encoder 2:\n",
      "create LoRA for Text Encoder: 264 modules.\n",
      "create LoRA for Text Encoder: 264 modules.\n",
      "create LoRA for Text Encoder: 264 modules.\n",
      "create LoRA for Text Encoder: 264 modules.\n",
      "create LoRA for U-Net: 722 modules.\n",
      "enable LoRA for U-Net\n",
      "create LoRA for U-Net: 722 modules.\n",
      "enable LoRA for U-Net\n",
      "learning rate is too low. If using D-Adaptation or Prodigy, set learning rate around 1.0 / 学習率が低すぎるようです。D-AdaptationまたはProdigyの使用時は1.0前後の値を指定してください: lr=0.0004\n",
      "recommend option: lr=1.0 / 推奨は1.0です\n",
      "use Prodigy optimizer | {}\n",
      "create LoRA for U-Net: 722 modules.\n",
      "enable LoRA for U-Net\n",
      "create LoRA for U-Net: 722 modules.\n",
      "enable LoRA for U-Net\n",
      "learning rate is too low. If using D-Adaptation or Prodigy, set learning rate around 1.0 / 学習率が低すぎるようです。D-AdaptationまたはProdigyの使用時は1.0前後の値を指定してください: lr=0.0004\n",
      "recommend option: lr=1.0 / 推奨は1.0です\n",
      "use Prodigy optimizer | {}\n",
      "learning rate is too low. If using D-Adaptation or Prodigy, set learning rate around 1.0 / 学習率が低すぎるようです。D-AdaptationまたはProdigyの使用時は1.0前後の値を指定してください: lr=0.0004\n",
      "recommend option: lr=1.0 / 推奨は1.0です\n",
      "use Prodigy optimizer | {}\n",
      "prepare optimizer, data loader etc.\n",
      "learning rate is too low. If using D-Adaptation or Prodigy, set learning rate around 1.0 / 学習率が低すぎるようです。D-AdaptationまたはProdigyの使用時は1.0前後の値を指定してください: lr=0.0004\n",
      "recommend option: lr=1.0 / 推奨は1.0です\n",
      "use Prodigy optimizer | {}\n",
      "override steps. steps for 10 epochs is / 指定エポックまでのステップ数: 500\n",
      "running training / 学習開始\n",
      "  num train images * repeats / 学習画像の数×繰り返し回数: 200\n",
      "  num reg images / 正則化画像の数: 0\n",
      "  num batches per epoch / 1epochのバッチ数: 50\n",
      "  num epochs / epoch数: 10\n",
      "  batch size per device / バッチサイズ: 1\n",
      "  gradient accumulation steps / 勾配を合計するステップ数 = 1\n",
      "  total optimization steps / 学習ステップ数: 500\n",
      "steps:   0%|                                            | 0/500 [00:00<?, ?it/s]\n",
      "epoch 1/10\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:339: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  and inp.query.storage().data_ptr() == inp.key.storage().data_ptr()\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:339: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  and inp.query.storage().data_ptr() == inp.key.storage().data_ptr()\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:339: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  and inp.query.storage().data_ptr() == inp.key.storage().data_ptr()\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:339: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  and inp.query.storage().data_ptr() == inp.key.storage().data_ptr()\n",
      "steps:  10%|██▎                    | 50/500 [01:17<11:34,  1.54s/it, loss=0.152]\n",
      "epoch 2/10\n",
      "steps:  20%|████▍                 | 100/500 [02:30<10:02,  1.51s/it, loss=0.133]\n",
      "epoch 3/10\n",
      "steps:  30%|██████▌               | 150/500 [03:44<08:44,  1.50s/it, loss=0.144]\n",
      "epoch 4/10\n",
      "steps:  40%|████████▊             | 200/500 [04:57<07:26,  1.49s/it, loss=0.127]\n",
      "epoch 5/10\n",
      "steps:  50%|███████████           | 250/500 [06:11<06:11,  1.49s/it, loss=0.147]\n",
      "epoch 6/10\n",
      "steps:  60%|██████████████▍         | 300/500 [07:26<04:57,  1.49s/it, loss=0.1]\n",
      "epoch 7/10\n",
      "steps:  70%|███████████████▍      | 350/500 [08:40<03:42,  1.49s/it, loss=0.143]\n",
      "epoch 8/10\n",
      "steps:  80%|█████████████████▌    | 400/500 [09:53<02:28,  1.48s/it, loss=0.156]\n",
      "epoch 9/10\n",
      "steps:  90%|███████████████████▊  | 450/500 [11:09<01:14,  1.49s/it, loss=0.113]\n",
      "epoch 10/10\n",
      "steps: 100%|██████████████████████| 500/500 [12:22<00:00,  1.49s/it, loss=0.111]\n",
      "generating sample images at step / サンプル画像生成 ステップ: 500\n",
      "generating sample images at step / サンプル画像生成 ステップ: 500\n",
      "generating sample images at step / サンプル画像生成 ステップ: 500\n",
      "generating sample images at step / サンプル画像生成 ステップ: 500\n",
      "\n",
      "\n",
      "\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ec2-user/SageMaker/sd-scripts/sdxl_train_network.py\", line 176, in <module>\n",
      "  File \"/home/ec2-user/SageMaker/sd-scripts/sdxl_train_network.py\", line 176, in <module>\n",
      "  File \"/home/ec2-user/SageMaker/sd-scripts/sdxl_train_network.py\", line 176, in <module>\n",
      "  File \"/home/ec2-user/SageMaker/sd-scripts/sdxl_train_network.py\", line 176, in <module>\n",
      "    trainer.train(args)\n",
      "  File \"/home/ec2-user/SageMaker/sd-scripts/train_network.py\", line 879, in train\n",
      "            trainer.train(args)trainer.train(args)trainer.train(args)\n",
      "\n",
      "\n",
      "  File \"/home/ec2-user/SageMaker/sd-scripts/train_network.py\", line 879, in train\n",
      "  File \"/home/ec2-user/SageMaker/sd-scripts/train_network.py\", line 879, in train\n",
      "  File \"/home/ec2-user/SageMaker/sd-scripts/train_network.py\", line 879, in train\n",
      "    self.sample_images(accelerator, args, epoch + 1, global_step, accelerator.device, vae, tokenizer, text_encoder, unet)\n",
      "  File \"/home/ec2-user/SageMaker/sd-scripts/sdxl_train_network.py\", line 160, in sample_images\n",
      "            self.sample_images(accelerator, args, epoch + 1, global_step, accelerator.device, vae, tokenizer, text_encoder, unet)self.sample_images(accelerator, args, epoch + 1, global_step, accelerator.device, vae, tokenizer, text_encoder, unet)self.sample_images(accelerator, args, epoch + 1, global_step, accelerator.device, vae, tokenizer, text_encoder, unet)\n",
      "\n",
      "    \n",
      "  File \"/home/ec2-user/SageMaker/sd-scripts/sdxl_train_network.py\", line 160, in sample_images\n",
      "  File \"/home/ec2-user/SageMaker/sd-scripts/sdxl_train_network.py\", line 160, in sample_images\n",
      "sdxl_train_util.sample_images(accelerator, args, epoch, global_step, device, vae, tokenizer, text_encoder, unet)  File \"/home/ec2-user/SageMaker/sd-scripts/sdxl_train_network.py\", line 160, in sample_images\n",
      "\n",
      "  File \"/home/ec2-user/SageMaker/sd-scripts/library/sdxl_train_util.py\", line 342, in sample_images\n",
      "            sdxl_train_util.sample_images(accelerator, args, epoch, global_step, device, vae, tokenizer, text_encoder, unet)sdxl_train_util.sample_images(accelerator, args, epoch, global_step, device, vae, tokenizer, text_encoder, unet)\n",
      "sdxl_train_util.sample_images(accelerator, args, epoch, global_step, device, vae, tokenizer, text_encoder, unet)\n",
      "  File \"/home/ec2-user/SageMaker/sd-scripts/library/sdxl_train_util.py\", line 342, in sample_images\n",
      "\n",
      "  File \"/home/ec2-user/SageMaker/sd-scripts/library/sdxl_train_util.py\", line 342, in sample_images\n",
      "  File \"/home/ec2-user/SageMaker/sd-scripts/library/sdxl_train_util.py\", line 342, in sample_images\n",
      "    return train_util.sample_images_common(SdxlStableDiffusionLongPromptWeightingPipeline, *args, **kwargs)\n",
      "  File \"/home/ec2-user/SageMaker/sd-scripts/library/train_util.py\", line 4355, in sample_images_common\n",
      "            return train_util.sample_images_common(SdxlStableDiffusionLongPromptWeightingPipeline, *args, **kwargs)return train_util.sample_images_common(SdxlStableDiffusionLongPromptWeightingPipeline, *args, **kwargs)\n",
      "\n",
      "return train_util.sample_images_common(SdxlStableDiffusionLongPromptWeightingPipeline, *args, **kwargs)  File \"/home/ec2-user/SageMaker/sd-scripts/library/train_util.py\", line 4355, in sample_images_common\n",
      "  File \"/home/ec2-user/SageMaker/sd-scripts/library/train_util.py\", line 4355, in sample_images_common\n",
      "\n",
      "  File \"/home/ec2-user/SageMaker/sd-scripts/library/train_util.py\", line 4355, in sample_images_common\n",
      "    if not os.path.isfile(args.sample_prompts):\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/genericpath.py\", line 30, in isfile\n",
      "    st = os.stat(path)\n",
      "TypeError: stat: path should be string, bytes, os.PathLike or integer, not NoneType\n",
      "            if not os.path.isfile(args.sample_prompts):if not os.path.isfile(args.sample_prompts):if not os.path.isfile(args.sample_prompts):\n",
      "\n",
      "\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/genericpath.py\", line 30, in isfile\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/genericpath.py\", line 30, in isfile\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/genericpath.py\", line 30, in isfile\n",
      "            st = os.stat(path)st = os.stat(path)st = os.stat(path)\n",
      "\n",
      "\n",
      "TypeErrorTypeErrorTypeError: : : stat: path should be string, bytes, os.PathLike or integer, not NoneTypestat: path should be string, bytes, os.PathLike or integer, not NoneType\n",
      "stat: path should be string, bytes, os.PathLike or integer, not NoneType\n",
      "\n",
      "steps: 100%|██████████████████████| 500/500 [12:22<00:00,  1.49s/it, loss=0.111]\n",
      "ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 2104) of binary: /home/ec2-user/anaconda3/envs/pytorch_p310/bin/python3.10\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/bin/accelerate\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/accelerate/commands/accelerate_cli.py\", line 45, in main\n",
      "    args.func(args)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/accelerate/commands/launch.py\", line 909, in launch_command\n",
      "    multi_gpu_launcher(args)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/accelerate/commands/launch.py\", line 604, in multi_gpu_launcher\n",
      "    distrib_run.run(args)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/distributed/run.py\", line 785, in run\n",
      "    elastic_launch(\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 134, in __call__\n",
      "    return launch_agent(self._config, self._entrypoint, list(args))\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 250, in launch_agent\n",
      "    raise ChildFailedError(\n",
      "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
      "============================================================\n",
      "sdxl_train_network.py FAILED\n",
      "------------------------------------------------------------\n",
      "Failures:\n",
      "[1]:\n",
      "  time      : 2023-08-28_18:27:44\n",
      "  host      : ip-172-16-42-158.us-west-2.compute.internal\n",
      "  rank      : 1 (local_rank: 1)\n",
      "  exitcode  : 1 (pid: 2105)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "[2]:\n",
      "  time      : 2023-08-28_18:27:44\n",
      "  host      : ip-172-16-42-158.us-west-2.compute.internal\n",
      "  rank      : 2 (local_rank: 2)\n",
      "  exitcode  : 1 (pid: 2106)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "[3]:\n",
      "  time      : 2023-08-28_18:27:44\n",
      "  host      : ip-172-16-42-158.us-west-2.compute.internal\n",
      "  rank      : 3 (local_rank: 3)\n",
      "  exitcode  : 1 (pid: 2107)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "------------------------------------------------------------\n",
      "Root Cause (first observed failure):\n",
      "[0]:\n",
      "  time      : 2023-08-28_18:27:44\n",
      "  host      : ip-172-16-42-158.us-west-2.compute.internal\n",
      "  rank      : 0 (local_rank: 0)\n",
      "  exitcode  : 1 (pid: 2104)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# !accelerate launch --multi_gpu --num_processes 4 sdxl_train_network.py \\\n",
    "#    --pretrained_model_name_or_path=\"stabilityai/stable-diffusion-xl-base-1.0\" \\\n",
    "#    --dataset_config=$DATASET_CONFIG \\\n",
    "#    --output_dir=$LORA_WEIGHT \\\n",
    "#    --network_module=\"networks.lora\" \\\n",
    "#    --max_train_epochs=10 \\\n",
    "#    --network_train_unet_only \\\n",
    "#    --learning_rate=4e-4 \\\n",
    "#    --lr_scheduler=\"constant\" \\\n",
    "#    --lr_scheduler_num_cycles=1 \\\n",
    "#    --network_dim=128 \\\n",
    "#    --network_alpha=32 \\\n",
    "#    --output_name=\"lora-aa\" \\\n",
    "#    --save_every_n_epochs=10 \\\n",
    "#    --mixed_precision=\"fp16\" \\\n",
    "#    --gradient_checkpointing \\\n",
    "#    --prior_loss_weight=1 \\\n",
    "#    --max_token_length=225 \\\n",
    "#    --save_model_as=\"safetensors\" \\\n",
    "#    --no_half_vae \\\n",
    "#    --xformers \\\n",
    "#    --optimizer_type=\"prodigy\" \\\n",
    "#    --sample_every_n_epochs=10 \\\n",
    "#    --sample_prompts=\"./sample_prompts.txt\" \\\n",
    "#    --sample_sampler=\"euler_a\" \\\n",
    "#    --logging_dir=\"./logs\" \\\n",
    "#    --log_with=\"all\" \\\n",
    "#    --wandb_api_key=\"245e51c77454f92bbb80ff882df76d02cfd7f4a5\" \\\n",
    "#    --log_tracker_name=\"lora-aa\" # register a user via https://wandb.ai and get an API key \n",
    "#    # --cache_text_encoder_outputs_to_disk \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a7f517-9cb3-4135-9366-426669b595ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
